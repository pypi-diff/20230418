# Comparing `tmp/bigdl_nano-2.3.0b20230417-py3-none-win_amd64.whl.zip` & `tmp/bigdl_nano-2.3.0b20230418-py3-none-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,226 +1,228 @@
-Zip file size: 2989081 bytes, number of entries: 224
--rw-------  2.0 unx      956 b- defN 23-Apr-17 02:22 bigdl/__init__.py
--rw-------  2.0 unx     1554 b- defN 23-Apr-17 02:22 bigdl/nano/__init__.py
--rw-------  2.0 unx      684 b- defN 23-Apr-17 02:22 bigdl/nano/openvino.py
--rw-------  2.0 unx      734 b- defN 23-Apr-17 02:22 bigdl/nano/automl/__init__.py
--rw-------  2.0 unx      633 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/__init__.py
--rw-------  2.0 unx     1888 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/backend.py
--rw-------  2.0 unx     8486 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/callgraph.py
--rw-------  2.0 unx    10528 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/config.py
--rw-------  2.0 unx    20683 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/decorator.py
--rw-------  2.0 unx     6431 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/search.py
--rw-------  2.0 unx    21436 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/space.py
--rw-------  2.0 unx      622 b- defN 23-Apr-17 02:22 bigdl/nano/automl/hpo/visualization.py
--rw-------  2.0 unx      624 b- defN 23-Apr-17 02:22 bigdl/nano/automl/pytorch/__init__.py
--rw-------  2.0 unx     5540 b- defN 23-Apr-17 02:22 bigdl/nano/automl/pytorch/_helper.py
--rw-------  2.0 unx    10273 b- defN 23-Apr-17 02:22 bigdl/nano/automl/pytorch/hposearcher.py
--rw-------  2.0 unx    11262 b- defN 23-Apr-17 02:22 bigdl/nano/automl/pytorch/objective.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/__init__.py
--rw-------  2.0 unx    16110 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/mixin.py
--rw-------  2.0 unx     4530 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/objective.py
--rw-------  2.0 unx     2379 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/keras/Model.py
--rw-------  2.0 unx     2876 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/keras/Sequential.py
--rw-------  2.0 unx      648 b- defN 23-Apr-17 02:22 bigdl/nano/automl/tf/keras/__init__.py
--rw-------  2.0 unx      648 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/__init__.py
--rw-------  2.0 unx     2128 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/edict.py
--rw-------  2.0 unx     1936 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/parallel.py
--rw-------  2.0 unx     1029 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/parallel_worker.py
--rw-------  2.0 unx     1263 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/proxy.py
--rw-------  2.0 unx     4318 b- defN 23-Apr-17 02:22 bigdl/nano/automl/utils/register_modules.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/__init__.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/automl/__init__.py
--rw-------  2.0 unx     2460 b- defN 23-Apr-17 02:22 bigdl/nano/deps/automl/hpo_api.py
--rw-------  2.0 unx     6876 b- defN 23-Apr-17 02:22 bigdl/nano/deps/automl/optuna_backend.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/horovod/__init__.py
--rw-------  2.0 unx     5304 b- defN 23-Apr-17 02:22 bigdl/nano/deps/horovod/distributed_utils_horovod.py
--rw-------  2.0 unx      934 b- defN 23-Apr-17 02:22 bigdl/nano/deps/horovod/horovod_api.py
--rw-------  2.0 unx     1125 b- defN 23-Apr-17 02:22 bigdl/nano/deps/horovod/horovod_worker.py
--rw-------  2.0 unx     2423 b- defN 23-Apr-17 02:22 bigdl/nano/deps/horovod/multiprocs_backend.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/__init__.py
--rw-------  2.0 unx     8275 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/ipex_api.py
--rw-------  2.0 unx     8121 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/ipex_inference_bf16_model.py
--rw-------  2.0 unx    15262 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/ipex_inference_model.py
--rw-------  2.0 unx     4444 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py
--rw-------  2.0 unx     7512 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ipex/ipex_quantization_model.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/__init__.py
--rw-------  2.0 unx     3907 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/inc_api.py
--rw-------  2.0 unx     9975 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/inc_api_2.py
--rw-------  2.0 unx     1013 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/core/__init__.py
--rw-------  2.0 unx     1856 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/core/base_metric.py
--rw-------  2.0 unx    10631 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/core/quantization.py
--rw-------  2.0 unx     1036 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/__init__.py
--rw-------  2.0 unx     1039 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/metric.py
--rw-------  2.0 unx     1101 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/quantization.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/pytorch/__init__.py
--rw-------  2.0 unx     1111 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/pytorch/metric.py
--rw-------  2.0 unx     2856 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/pytorch/quantization.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/tensorflow/__init__.py
--rw-------  2.0 unx     1146 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/tensorflow/metric.py
--rw-------  2.0 unx     2621 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/onnx/tensorflow/quantization.py
--rw-------  2.0 unx      632 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/pytorch/__init__.py
--rw-------  2.0 unx      902 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/pytorch/metric.py
--rw-------  2.0 unx     2867 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/pytorch/quantization.py
--rw-------  2.0 unx     3701 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/pytorch/quantized_model.py
--rw-------  2.0 unx     2233 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/pytorch/utils.py
--rw-------  2.0 unx      635 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/tensorflow/__init__.py
--rw-------  2.0 unx     1088 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/tensorflow/metric.py
--rw-------  2.0 unx     4219 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/tensorflow/model.py
--rw-------  2.0 unx     3007 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/tensorflow/quantization.py
--rw-------  2.0 unx      729 b- defN 23-Apr-17 02:22 bigdl/nano/deps/neural_compressor/tensorflow/utils.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/__init__.py
--rw-------  2.0 unx     4990 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/onnxruntime_api.py
--rw-------  2.0 unx      932 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/core/__init__.py
--rw-------  2.0 unx     3655 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/core/onnxruntime_model.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/pytorch/__init__.py
--rw-------  2.0 unx     7854 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/tensorflow/__init__.py
--rw-------  2.0 unx     7852 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxruntime/tensorflow/model.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxsim/__init__.py
--rw-------  2.0 unx     1105 b- defN 23-Apr-17 02:22 bigdl/nano/deps/onnxsim/onnxsim_api.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/__init__.py
--rw-------  2.0 unx     6986 b- defN 23-Apr-17 08:03 bigdl/nano/deps/openvino/openvino_api.py
--rw-------  2.0 unx      872 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/core/__init__.py
--rw-------  2.0 unx     2129 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/core/metric.py
--rw-------  2.0 unx    14225 b- defN 23-Apr-17 08:03 bigdl/nano/deps/openvino/core/model.py
--rw-------  2.0 unx     4075 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/core/utils.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/pytorch/__init__.py
--rw-------  2.0 unx     1519 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/pytorch/dataloader.py
--rw-------  2.0 unx     1668 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/pytorch/metric.py
--rw-------  2.0 unx    13001 b- defN 23-Apr-17 08:03 bigdl/nano/deps/openvino/pytorch/model.py
--rw-------  2.0 unx     2616 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/pytorch/utils.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/tf/__init__.py
--rw-------  2.0 unx     1845 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/tf/dataloader.py
--rw-------  2.0 unx     1499 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/tf/metric.py
--rw-------  2.0 unx    10317 b- defN 23-Apr-17 08:03 bigdl/nano/deps/openvino/tf/model.py
--rw-------  2.0 unx     1676 b- defN 23-Apr-17 02:22 bigdl/nano/deps/openvino/tf/utils.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ray/__init__.py
--rw-------  2.0 unx     1265 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ray/ray_api.py
--rw-------  2.0 unx     1530 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ray/ray_backend.py
--rw-------  2.0 unx    18650 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ray/ray_distributed.py
--rw-------  2.0 unx     3705 b- defN 23-Apr-17 02:22 bigdl/nano/deps/ray/ray_envbase.py
--rw-------  2.0 unx      618 b- defN 23-Apr-17 02:22 bigdl/nano/k8s/__init__.py
--rw-------  2.0 unx    13183 b- defN 23-Apr-17 02:22 bigdl/nano/k8s/bigdl_submit.py
--rwx------  2.0 unx  5460184 b- defN 23-Apr-17 14:19 bigdl/nano/libs/libjemalloc.so
--rwx------  2.0 unx  1432624 b- defN 23-Apr-17 14:19 bigdl/nano/libs/libtcmalloc.so
--rwx------  2.0 unx   918296 b- defN 23-Apr-17 14:19 bigdl/nano/libs/libturbojpeg.so.0.2.0
--rw-------  2.0 unx     2113 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/__init__.py
--rw-------  2.0 unx     5657 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/context_manager.py
--rw-------  2.0 unx     3245 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/dispatcher.py
--rw-------  2.0 unx     5818 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/lightning.py
--rw-------  2.0 unx     3063 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/model.py
--rw-------  2.0 unx    18937 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/torch_nano.py
--rw-------  2.0 unx      706 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/algorithms/__init__.py
--rw-------  2.0 unx     1235 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/algorithms/selective_backprop/__init__.py
--rw-------  2.0 unx    12005 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/algorithms/selective_backprop/selective_backprop.py
--rw-------  2.0 unx      609 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/amp/__init__.py
--rw-------  2.0 unx      805 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/amp/amp_api.py
--rw-------  2.0 unx     9569 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/amp/bfloat16.py
--rw-------  2.0 unx      640 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/encryption/__init__.py
--rw-------  2.0 unx     6480 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/encryption/encryption.py
--rw-------  2.0 unx      661 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/inference/__init__.py
--rw-------  2.0 unx     4231 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/inference/multi_instance.py
--rw-------  2.0 unx    86860 b- defN 23-Apr-17 08:03 bigdl/nano/pytorch/inference/optimizer.py
--rw-------  2.0 unx     4593 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/inference/pipeline.py
--rw-------  2.0 unx     3015 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/low_precision/jit_int8_api.py
--rw-------  2.0 unx     9651 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/low_precision/jit_int8_model.py
--rw-------  2.0 unx      646 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/optim/__init__.py
--rw-------  2.0 unx     8351 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/optim/sparseadam.py
--rw-------  2.0 unx      804 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/__init__.py
--rw-------  2.0 unx      627 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/dtype_patching/__init__.py
--rw-------  2.0 unx     4206 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/dtype_patching/dtype_patching.py
--rw-------  2.0 unx      637 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/encryption_patching/__init__.py
--rw-------  2.0 unx     1854 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/encryption_patching/encryption_patching.py
--rw-------  2.0 unx      650 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/gpu_cpu/__init__.py
--rw-------  2.0 unx     7863 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/patching/gpu_cpu/gpu_cpu.py
--rw-------  2.0 unx      828 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/__init__.py
--rw-------  2.0 unx    13954 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/ddp_spawn.py
--rw-------  2.0 unx     6781 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/ddp_subprocess.py
--rw-------  2.0 unx     4706 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/ipex_strategy.py
--rw-------  2.0 unx     8908 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/k8s.py
--rw-------  2.0 unx     2059 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/strategies/worker.py
--rw-------  2.0 unx    23922 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/trainer/Trainer.py
--rw-------  2.0 unx      617 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/trainer/__init__.py
--rw-------  2.0 unx      875 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/datasets/__init__.py
--rw-------  2.0 unx     7094 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/datasets/datasets.py
--rw-------  2.0 unx     4528 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/datasets/oxfordpet_datasets.py
--rw-------  2.0 unx      672 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/models/__init__.py
--rw-------  2.0 unx     2133 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/models/_utils.py
--rw-------  2.0 unx     2455 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/models/classifier.py
--rw-------  2.0 unx     8900 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/models/vision.py
--rw-------  2.0 unx      613 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/transforms/__init__.py
--rw-------  2.0 unx    31603 b- defN 23-Apr-17 02:22 bigdl/nano/pytorch/vision/transforms/transforms.py
--rw-------  2.0 unx     1618 b- defN 23-Apr-17 02:22 bigdl/nano/tf/__init__.py
--rw-------  2.0 unx     4369 b- defN 23-Apr-17 02:22 bigdl/nano/tf/dispatcher.py
--rw-------  2.0 unx     4894 b- defN 23-Apr-17 02:22 bigdl/nano/tf/model.py
--rw-------  2.0 unx     1294 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/Model.py
--rw-------  2.0 unx     1076 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/Sequential.py
--rw-------  2.0 unx      845 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/__init__.py
--rw-------  2.0 unx    10780 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/customized_training_utils.py
--rw-------  2.0 unx     4929 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/distributed_utils.py
--rw-------  2.0 unx    11245 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/inference_utils.py
--rw-------  2.0 unx     1244 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/inheritance_utils.py
--rw-------  2.0 unx     5393 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/training_utils.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/activations/__init__.py
--rw-------  2.0 unx      609 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/amp/__init__.py
--rw-------  2.0 unx      798 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/amp/amp_api.py
--rw-------  2.0 unx     2036 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/amp/bfloat16.py
--rw-------  2.0 unx     1858 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/inference/_worker.py
--rw-------  2.0 unx    48489 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/inference/optimizer.py
--rw-------  2.0 unx      647 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/layers/__init__.py
--rw-------  2.0 unx     4057 b- defN 23-Apr-17 02:22 bigdl/nano/tf/keras/layers/embeddings.py
--rw-------  2.0 unx      664 b- defN 23-Apr-17 02:22 bigdl/nano/tf/optimizers/__init__.py
--rw-------  2.0 unx     6441 b- defN 23-Apr-17 02:22 bigdl/nano/tf/optimizers/sparse_adam.py
--rw-------  2.0 unx      586 b- defN 23-Apr-17 02:22 bigdl/nano/utils/__init__.py
--rw-------  2.0 unx     1556 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/__init__.py
--rw-------  2.0 unx     1782 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/affinity.py
--rw-------  2.0 unx      746 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/backend.py
--rw-------  2.0 unx     2072 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/checker.py
--rw-------  2.0 unx     3522 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/cpuinfo.py
--rw-------  2.0 unx     1270 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/decorator.py
--rw-------  2.0 unx     3429 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/env.py
--rw-------  2.0 unx      790 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/function.py
--rw-------  2.0 unx     1057 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/inspect.py
--rw-------  2.0 unx     1378 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/log4Error.py
--rw-------  2.0 unx     1248 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/log4Warning.py
--rw-------  2.0 unx     2640 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/model.py
--rw-------  2.0 unx     5227 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/schedule.py
--rw-------  2.0 unx     3278 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/subprocess.py
--rw-------  2.0 unx     1688 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/version.py
--rw-------  2.0 unx      967 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/__init__.py
--rw-------  2.0 unx     3536 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/acceleration_option.py
--rw-------  2.0 unx     3970 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/format.py
--rw-------  2.0 unx     4422 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/latency.py
--rw-------  2.0 unx      709 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/metric.py
--rw-------  2.0 unx     8336 b- defN 23-Apr-17 02:22 bigdl/nano/utils/common/optimizer/optimizer.py
--rw-------  2.0 unx     1870 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/__init__.py
--rw-------  2.0 unx     1100 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/attributes.py
--rw-------  2.0 unx     4527 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/channel_last.py
--rw-------  2.0 unx     1807 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/check_deps.py
--rw-------  2.0 unx     3358 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/convert.py
--rw-------  2.0 unx     6234 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/dataloader.py
--rw-------  2.0 unx     1321 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/dataset.py
--rw-------  2.0 unx     3881 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/input_sample.py
--rw-------  2.0 unx     4952 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/inspect.py
--rw-------  2.0 unx     6270 b- defN 23-Apr-17 08:03 bigdl/nano/utils/pytorch/load.py
--rw-------  2.0 unx     1391 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/metric.py
--rw-------  2.0 unx     5531 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/model_info.py
--rw-------  2.0 unx     2878 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/save.py
--rw-------  2.0 unx     1189 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/version.py
--rw-------  2.0 unx     1012 b- defN 23-Apr-17 02:22 bigdl/nano/utils/pytorch/xpu.py
--rw-------  2.0 unx     1057 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/__init__.py
--rw-------  2.0 unx     4300 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/attributes.py
--rw-------  2.0 unx     2816 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/backend.py
--rw-------  2.0 unx     7421 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/data.py
--rw-------  2.0 unx     2582 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/preprocess.py
--rw-------  2.0 unx     1347 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/subprocess_worker.py
--rw-------  2.0 unx      823 b- defN 23-Apr-17 02:22 bigdl/nano/utils/tf/version.py
--rwxr-xr-x  2.0 unx    12068 b- defN 23-Apr-17 02:22 bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init
--rwxr-xr-x  2.0 unx      909 b- defN 23-Apr-17 02:22 bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init.ps1
--rwxr-xr-x  2.0 unx      214 b- defN 23-Apr-17 02:22 bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env
--rwxr-xr-x  2.0 unx      127 b- defN 23-Apr-17 02:22 bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env.ps1
--rw-------  2.0 unx    10112 b- defN 23-Apr-17 14:19 bigdl_nano-2.3.0b20230417.dist-info/METADATA
--rw-------  2.0 unx       98 b- defN 23-Apr-17 14:19 bigdl_nano-2.3.0b20230417.dist-info/WHEEL
--rw-------  2.0 unx       54 b- defN 23-Apr-17 14:19 bigdl_nano-2.3.0b20230417.dist-info/entry_points.txt
--rw-------  2.0 unx        6 b- defN 23-Apr-17 14:19 bigdl_nano-2.3.0b20230417.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    21777 b- defN 23-Apr-17 14:19 bigdl_nano-2.3.0b20230417.dist-info/RECORD
-224 files, 8816779 bytes uncompressed, 2953835 bytes compressed:  66.5%
+Zip file size: 2993492 bytes, number of entries: 226
+-rw-r--r--  2.0 unx      956 b- defN 22-Oct-25 01:39 bigdl/__init__.py
+-rw-r--r--  2.0 unx     1554 b- defN 23-Feb-09 11:06 bigdl/nano/__init__.py
+-rw-r--r--  2.0 unx      684 b- defN 22-Oct-25 01:39 bigdl/nano/openvino.py
+-rw-r--r--  2.0 unx      734 b- defN 22-Oct-25 01:39 bigdl/nano/automl/__init__.py
+-rw-r--r--  2.0 unx      633 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/__init__.py
+-rw-r--r--  2.0 unx     1888 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/backend.py
+-rw-r--r--  2.0 unx     8486 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/callgraph.py
+-rw-r--r--  2.0 unx    10528 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/config.py
+-rw-r--r--  2.0 unx    20683 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/decorator.py
+-rw-r--r--  2.0 unx     6431 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/search.py
+-rw-r--r--  2.0 unx    21436 b- defN 23-Feb-09 11:06 bigdl/nano/automl/hpo/space.py
+-rw-r--r--  2.0 unx      622 b- defN 22-Oct-25 01:39 bigdl/nano/automl/hpo/visualization.py
+-rw-r--r--  2.0 unx      624 b- defN 22-Oct-25 01:39 bigdl/nano/automl/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5540 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/_helper.py
+-rw-r--r--  2.0 unx    10273 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/hposearcher.py
+-rw-r--r--  2.0 unx    11262 b- defN 23-Feb-09 11:06 bigdl/nano/automl/pytorch/objective.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/__init__.py
+-rw-r--r--  2.0 unx    16110 b- defN 23-Feb-09 11:06 bigdl/nano/automl/tf/mixin.py
+-rw-r--r--  2.0 unx     4530 b- defN 23-Feb-09 11:06 bigdl/nano/automl/tf/objective.py
+-rw-r--r--  2.0 unx     2379 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/Model.py
+-rw-r--r--  2.0 unx     2876 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/Sequential.py
+-rw-r--r--  2.0 unx      648 b- defN 22-Oct-25 01:39 bigdl/nano/automl/tf/keras/__init__.py
+-rw-r--r--  2.0 unx      648 b- defN 22-Oct-25 01:39 bigdl/nano/automl/utils/__init__.py
+-rw-r--r--  2.0 unx     2128 b- defN 22-Oct-25 01:39 bigdl/nano/automl/utils/edict.py
+-rw-r--r--  2.0 unx     1936 b- defN 23-Apr-11 11:07 bigdl/nano/automl/utils/parallel.py
+-rw-r--r--  2.0 unx     1029 b- defN 23-Apr-11 11:07 bigdl/nano/automl/utils/parallel_worker.py
+-rw-r--r--  2.0 unx     1263 b- defN 23-Feb-09 11:06 bigdl/nano/automl/utils/proxy.py
+-rw-r--r--  2.0 unx     4318 b- defN 23-Feb-09 11:06 bigdl/nano/automl/utils/register_modules.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/automl/__init__.py
+-rw-r--r--  2.0 unx     2460 b- defN 22-Oct-25 01:39 bigdl/nano/deps/automl/hpo_api.py
+-rw-r--r--  2.0 unx     6876 b- defN 23-Feb-09 11:06 bigdl/nano/deps/automl/optuna_backend.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/__init__.py
+-rw-r--r--  2.0 unx     5304 b- defN 23-Feb-09 11:06 bigdl/nano/deps/horovod/distributed_utils_horovod.py
+-rw-r--r--  2.0 unx      934 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/horovod_api.py
+-rw-r--r--  2.0 unx     1125 b- defN 22-Oct-25 01:39 bigdl/nano/deps/horovod/horovod_worker.py
+-rw-r--r--  2.0 unx     2423 b- defN 23-Feb-09 11:06 bigdl/nano/deps/horovod/multiprocs_backend.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ipex/__init__.py
+-rw-r--r--  2.0 unx     9510 b- defN 23-Apr-18 11:06 bigdl/nano/deps/ipex/ipex_api.py
+-rw-r--r--  2.0 unx     8545 b- defN 23-Apr-18 11:06 bigdl/nano/deps/ipex/ipex_inference_bf16_model.py
+-rw-r--r--  2.0 unx    14090 b- defN 23-Apr-18 11:06 bigdl/nano/deps/ipex/ipex_inference_model.py
+-rw-r--r--  2.0 unx     4444 b- defN 23-Apr-07 11:07 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py
+-rw-r--r--  2.0 unx     8087 b- defN 23-Apr-18 11:06 bigdl/nano/deps/ipex/ipex_quantization_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/__init__.py
+-rw-r--r--  2.0 unx     3907 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/inc_api.py
+-rw-r--r--  2.0 unx     9975 b- defN 23-Apr-13 11:07 bigdl/nano/deps/neural_compressor/inc_api_2.py
+-rw-r--r--  2.0 unx     1013 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/__init__.py
+-rw-r--r--  2.0 unx     1856 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/base_metric.py
+-rw-r--r--  2.0 unx    10631 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/core/quantization.py
+-rw-r--r--  2.0 unx     1036 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/onnx/__init__.py
+-rw-r--r--  2.0 unx     1039 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/metric.py
+-rw-r--r--  2.0 unx     1101 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/quantization.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1111 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/onnx/pytorch/metric.py
+-rw-r--r--  2.0 unx     2856 b- defN 22-Dec-06 11:06 bigdl/nano/deps/neural_compressor/onnx/pytorch/quantization.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Nov-10 05:41 bigdl/nano/deps/neural_compressor/onnx/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     1146 b- defN 22-Nov-10 05:41 bigdl/nano/deps/neural_compressor/onnx/tensorflow/metric.py
+-rw-r--r--  2.0 unx     2621 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/onnx/tensorflow/quantization.py
+-rw-r--r--  2.0 unx      632 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/pytorch/__init__.py
+-rw-r--r--  2.0 unx      902 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/pytorch/metric.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Feb-19 03:08 bigdl/nano/deps/neural_compressor/pytorch/quantization.py
+-rw-r--r--  2.0 unx     3701 b- defN 23-Feb-27 07:35 bigdl/nano/deps/neural_compressor/pytorch/quantized_model.py
+-rw-r--r--  2.0 unx     2233 b- defN 23-Feb-09 11:06 bigdl/nano/deps/neural_compressor/pytorch/utils.py
+-rw-r--r--  2.0 unx      635 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     1088 b- defN 23-Feb-01 11:07 bigdl/nano/deps/neural_compressor/tensorflow/metric.py
+-rw-r--r--  2.0 unx     4219 b- defN 23-Mar-13 11:07 bigdl/nano/deps/neural_compressor/tensorflow/model.py
+-rw-r--r--  2.0 unx     3007 b- defN 23-Mar-09 11:07 bigdl/nano/deps/neural_compressor/tensorflow/quantization.py
+-rw-r--r--  2.0 unx      729 b- defN 22-Oct-25 01:39 bigdl/nano/deps/neural_compressor/tensorflow/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/__init__.py
+-rw-r--r--  2.0 unx     4990 b- defN 23-Mar-07 11:07 bigdl/nano/deps/onnxruntime/onnxruntime_api.py
+-rw-r--r--  2.0 unx      932 b- defN 23-Feb-09 11:06 bigdl/nano/deps/onnxruntime/core/__init__.py
+-rw-r--r--  2.0 unx     3655 b- defN 23-Apr-13 11:07 bigdl/nano/deps/onnxruntime/core/onnxruntime_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/pytorch/__init__.py
+-rw-r--r--  2.0 unx     9530 b- defN 23-Apr-18 11:06 bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/onnxruntime/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     7852 b- defN 23-Mar-13 11:07 bigdl/nano/deps/onnxruntime/tensorflow/model.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 12:02 bigdl/nano/deps/onnxsim/__init__.py
+-rw-r--r--  2.0 unx     1105 b- defN 22-Oct-25 12:02 bigdl/nano/deps/onnxsim/onnxsim_api.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/__init__.py
+-rw-r--r--  2.0 unx     6986 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/openvino_api.py
+-rw-r--r--  2.0 unx      872 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/core/__init__.py
+-rw-r--r--  2.0 unx     2129 b- defN 22-Dec-29 11:06 bigdl/nano/deps/openvino/core/metric.py
+-rw-r--r--  2.0 unx    14225 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/core/model.py
+-rw-r--r--  2.0 unx     4075 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/core/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1519 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/pytorch/dataloader.py
+-rw-r--r--  2.0 unx     1668 b- defN 23-Mar-03 11:07 bigdl/nano/deps/openvino/pytorch/metric.py
+-rw-r--r--  2.0 unx    14656 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/pytorch/model.py
+-rw-r--r--  2.0 unx     2616 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/pytorch/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/openvino/tf/__init__.py
+-rw-r--r--  2.0 unx     1845 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/tf/dataloader.py
+-rw-r--r--  2.0 unx     1499 b- defN 23-Feb-09 11:06 bigdl/nano/deps/openvino/tf/metric.py
+-rw-r--r--  2.0 unx    10317 b- defN 23-Apr-18 11:06 bigdl/nano/deps/openvino/tf/model.py
+-rw-r--r--  2.0 unx     1676 b- defN 23-Jan-06 02:27 bigdl/nano/deps/openvino/tf/utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ray/__init__.py
+-rw-r--r--  2.0 unx     1265 b- defN 23-Feb-09 11:06 bigdl/nano/deps/ray/ray_api.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Feb-09 11:06 bigdl/nano/deps/ray/ray_backend.py
+-rw-r--r--  2.0 unx    18650 b- defN 23-Mar-01 03:16 bigdl/nano/deps/ray/ray_distributed.py
+-rw-r--r--  2.0 unx     3705 b- defN 22-Oct-25 01:39 bigdl/nano/deps/ray/ray_envbase.py
+-rw-r--r--  2.0 unx      618 b- defN 22-Oct-25 01:39 bigdl/nano/k8s/__init__.py
+-rw-r--r--  2.0 unx    13183 b- defN 23-Feb-09 11:06 bigdl/nano/k8s/bigdl_submit.py
+-rwxr--r--  2.0 unx  5460184 b- defN 23-Apr-18 11:08 bigdl/nano/libs/libjemalloc.so
+-rwxr--r--  2.0 unx  1432624 b- defN 23-Apr-18 11:09 bigdl/nano/libs/libtcmalloc.so
+-rwxr--r--  2.0 unx   918296 b- defN 23-Apr-18 11:09 bigdl/nano/libs/libturbojpeg.so.0.2.0
+-rw-r--r--  2.0 unx     2113 b- defN 23-Mar-09 11:07 bigdl/nano/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5657 b- defN 23-Apr-07 11:07 bigdl/nano/pytorch/context_manager.py
+-rw-r--r--  2.0 unx     3245 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/dispatcher.py
+-rw-r--r--  2.0 unx     5818 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/lightning.py
+-rw-r--r--  2.0 unx     3063 b- defN 23-Apr-13 11:07 bigdl/nano/pytorch/model.py
+-rw-r--r--  2.0 unx    18937 b- defN 23-Mar-01 07:03 bigdl/nano/pytorch/torch_nano.py
+-rw-r--r--  2.0 unx      706 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/algorithms/__init__.py
+-rw-r--r--  2.0 unx     1235 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/algorithms/selective_backprop/__init__.py
+-rw-r--r--  2.0 unx    12005 b- defN 23-Feb-15 11:07 bigdl/nano/pytorch/algorithms/selective_backprop/selective_backprop.py
+-rw-r--r--  2.0 unx      609 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/amp/__init__.py
+-rw-r--r--  2.0 unx      805 b- defN 22-Nov-22 07:23 bigdl/nano/pytorch/amp/amp_api.py
+-rw-r--r--  2.0 unx     9569 b- defN 23-Mar-14 11:07 bigdl/nano/pytorch/amp/bfloat16.py
+-rw-r--r--  2.0 unx      640 b- defN 23-Jan-09 11:06 bigdl/nano/pytorch/encryption/__init__.py
+-rw-r--r--  2.0 unx     6480 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/encryption/encryption.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Feb-23 11:07 bigdl/nano/pytorch/inference/__init__.py
+-rw-r--r--  2.0 unx     4231 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/inference/multi_instance.py
+-rw-r--r--  2.0 unx    88302 b- defN 23-Apr-18 11:06 bigdl/nano/pytorch/inference/optimizer.py
+-rw-r--r--  2.0 unx     4593 b- defN 23-Feb-27 07:35 bigdl/nano/pytorch/inference/pipeline.py
+-rw-r--r--  2.0 unx     3430 b- defN 23-Apr-18 11:06 bigdl/nano/pytorch/low_precision/jit_int8_api.py
+-rw-r--r--  2.0 unx    11050 b- defN 23-Apr-18 11:06 bigdl/nano/pytorch/low_precision/jit_int8_model.py
+-rw-r--r--  2.0 unx      646 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/optim/__init__.py
+-rw-r--r--  2.0 unx     8351 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/optim/sparseadam.py
+-rw-r--r--  2.0 unx      804 b- defN 23-Jan-06 11:06 bigdl/nano/pytorch/patching/__init__.py
+-rw-r--r--  2.0 unx      627 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/patching/dtype_patching/__init__.py
+-rw-r--r--  2.0 unx     4206 b- defN 23-Feb-09 11:06 bigdl/nano/pytorch/patching/dtype_patching/dtype_patching.py
+-rw-r--r--  2.0 unx      637 b- defN 23-Jan-06 11:06 bigdl/nano/pytorch/patching/encryption_patching/__init__.py
+-rw-r--r--  2.0 unx     1854 b- defN 23-Jan-11 11:06 bigdl/nano/pytorch/patching/encryption_patching/encryption_patching.py
+-rw-r--r--  2.0 unx      650 b- defN 22-Dec-16 11:06 bigdl/nano/pytorch/patching/gpu_cpu/__init__.py
+-rw-r--r--  2.0 unx     7863 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/patching/gpu_cpu/gpu_cpu.py
+-rw-r--r--  2.0 unx      828 b- defN 22-Dec-26 11:06 bigdl/nano/pytorch/strategies/__init__.py
+-rw-r--r--  2.0 unx    13954 b- defN 23-Mar-01 03:16 bigdl/nano/pytorch/strategies/ddp_spawn.py
+-rw-r--r--  2.0 unx     6781 b- defN 23-Apr-11 11:07 bigdl/nano/pytorch/strategies/ddp_subprocess.py
+-rw-r--r--  2.0 unx     4706 b- defN 23-Feb-13 11:07 bigdl/nano/pytorch/strategies/ipex_strategy.py
+-rw-r--r--  2.0 unx     8908 b- defN 23-Mar-01 03:16 bigdl/nano/pytorch/strategies/k8s.py
+-rw-r--r--  2.0 unx     2059 b- defN 23-Apr-11 11:07 bigdl/nano/pytorch/strategies/worker.py
+-rw-r--r--  2.0 unx    23922 b- defN 23-Mar-01 07:03 bigdl/nano/pytorch/trainer/Trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/trainer/__init__.py
+-rw-r--r--  2.0 unx      875 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/vision/datasets/__init__.py
+-rw-r--r--  2.0 unx     7094 b- defN 23-Feb-21 06:40 bigdl/nano/pytorch/vision/datasets/datasets.py
+-rw-r--r--  2.0 unx     4528 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/datasets/oxfordpet_datasets.py
+-rw-r--r--  2.0 unx      672 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/__init__.py
+-rw-r--r--  2.0 unx     2133 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/_utils.py
+-rw-r--r--  2.0 unx     2455 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/classifier.py
+-rw-r--r--  2.0 unx     8900 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/models/vision.py
+-rw-r--r--  2.0 unx      613 b- defN 22-Oct-25 01:39 bigdl/nano/pytorch/vision/transforms/__init__.py
+-rw-r--r--  2.0 unx    31603 b- defN 23-Feb-19 03:08 bigdl/nano/pytorch/vision/transforms/transforms.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Feb-18 11:06 bigdl/nano/tf/__init__.py
+-rw-r--r--  2.0 unx     4369 b- defN 23-Feb-09 11:06 bigdl/nano/tf/dispatcher.py
+-rw-r--r--  2.0 unx     4894 b- defN 23-Mar-13 11:07 bigdl/nano/tf/model.py
+-rw-r--r--  2.0 unx     1294 b- defN 23-Jan-11 11:06 bigdl/nano/tf/keras/Model.py
+-rw-r--r--  2.0 unx     1076 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/Sequential.py
+-rw-r--r--  2.0 unx      845 b- defN 23-Apr-04 11:07 bigdl/nano/tf/keras/__init__.py
+-rw-r--r--  2.0 unx    10780 b- defN 23-Apr-07 11:07 bigdl/nano/tf/keras/customized_training_utils.py
+-rw-r--r--  2.0 unx     4929 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/distributed_utils.py
+-rw-r--r--  2.0 unx    11245 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/inference_utils.py
+-rw-r--r--  2.0 unx     1244 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/inheritance_utils.py
+-rw-r--r--  2.0 unx     5393 b- defN 23-Feb-09 11:06 bigdl/nano/tf/keras/training_utils.py
+-rw-r--r--  2.0 unx      586 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/activations/__init__.py
+-rw-r--r--  2.0 unx      609 b- defN 23-Jan-10 11:06 bigdl/nano/tf/keras/amp/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 23-Feb-02 11:06 bigdl/nano/tf/keras/amp/amp_api.py
+-rw-r--r--  2.0 unx     2036 b- defN 23-Mar-24 11:07 bigdl/nano/tf/keras/amp/bfloat16.py
+-rw-r--r--  2.0 unx     1858 b- defN 23-Feb-14 11:07 bigdl/nano/tf/keras/inference/_worker.py
+-rw-r--r--  2.0 unx    48489 b- defN 23-Mar-24 11:07 bigdl/nano/tf/keras/inference/optimizer.py
+-rw-r--r--  2.0 unx      647 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/layers/__init__.py
+-rw-r--r--  2.0 unx     4057 b- defN 22-Oct-25 01:39 bigdl/nano/tf/keras/layers/embeddings.py
+-rw-r--r--  2.0 unx      664 b- defN 22-Oct-25 01:39 bigdl/nano/tf/optimizers/__init__.py
+-rw-r--r--  2.0 unx     6441 b- defN 23-Feb-19 03:08 bigdl/nano/tf/optimizers/sparse_adam.py
+-rw-r--r--  2.0 unx      586 b- defN 23-Feb-09 11:06 bigdl/nano/utils/__init__.py
+-rw-r--r--  2.0 unx     1556 b- defN 23-Apr-12 06:44 bigdl/nano/utils/common/__init__.py
+-rw-r--r--  2.0 unx     1782 b- defN 23-Feb-17 11:06 bigdl/nano/utils/common/affinity.py
+-rw-r--r--  2.0 unx      746 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/backend.py
+-rw-r--r--  2.0 unx     2072 b- defN 23-Apr-11 11:07 bigdl/nano/utils/common/checker.py
+-rw-r--r--  2.0 unx     3522 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/cpuinfo.py
+-rw-r--r--  2.0 unx     1270 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/decorator.py
+-rw-r--r--  2.0 unx     3429 b- defN 23-Feb-27 07:35 bigdl/nano/utils/common/env.py
+-rw-r--r--  2.0 unx      790 b- defN 23-Apr-12 06:44 bigdl/nano/utils/common/function.py
+-rw-r--r--  2.0 unx     1057 b- defN 23-Feb-09 11:06 bigdl/nano/utils/common/inspect.py
+-rw-r--r--  2.0 unx     1378 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/log4Error.py
+-rw-r--r--  2.0 unx     1248 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/log4Warning.py
+-rw-r--r--  2.0 unx     2640 b- defN 23-Feb-21 03:10 bigdl/nano/utils/common/model.py
+-rw-r--r--  2.0 unx     5227 b- defN 23-Feb-10 11:06 bigdl/nano/utils/common/schedule.py
+-rw-r--r--  2.0 unx     3278 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/subprocess.py
+-rw-r--r--  2.0 unx     1688 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/version.py
+-rw-r--r--  2.0 unx      967 b- defN 23-Mar-15 11:07 bigdl/nano/utils/common/optimizer/__init__.py
+-rw-r--r--  2.0 unx     3536 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/acceleration_option.py
+-rw-r--r--  2.0 unx     3970 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/format.py
+-rw-r--r--  2.0 unx     4422 b- defN 23-Mar-15 11:07 bigdl/nano/utils/common/optimizer/latency.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Feb-09 11:07 bigdl/nano/utils/common/optimizer/metric.py
+-rw-r--r--  2.0 unx     8336 b- defN 23-Mar-07 11:07 bigdl/nano/utils/common/optimizer/optimizer.py
+-rw-r--r--  2.0 unx     1939 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx     1100 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/attributes.py
+-rw-r--r--  2.0 unx     4527 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/channel_last.py
+-rw-r--r--  2.0 unx     1807 b- defN 23-Mar-01 07:03 bigdl/nano/utils/pytorch/check_deps.py
+-rw-r--r--  2.0 unx     3358 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/convert.py
+-rw-r--r--  2.0 unx     6234 b- defN 23-Feb-23 11:07 bigdl/nano/utils/pytorch/dataloader.py
+-rw-r--r--  2.0 unx     1321 b- defN 23-Feb-09 11:07 bigdl/nano/utils/pytorch/dataset.py
+-rw-r--r--  2.0 unx     3881 b- defN 23-Feb-10 11:06 bigdl/nano/utils/pytorch/input_sample.py
+-rw-r--r--  2.0 unx     4952 b- defN 23-Feb-19 03:08 bigdl/nano/utils/pytorch/inspect.py
+-rw-r--r--  2.0 unx     2595 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/jit_method.py
+-rw-r--r--  2.0 unx     6270 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/load.py
+-rw-r--r--  2.0 unx     4739 b- defN 23-Apr-18 11:06 bigdl/nano/utils/pytorch/metadata.py
+-rw-r--r--  2.0 unx     1391 b- defN 23-Feb-09 11:07 bigdl/nano/utils/pytorch/metric.py
+-rw-r--r--  2.0 unx     5531 b- defN 23-Mar-03 11:07 bigdl/nano/utils/pytorch/model_info.py
+-rw-r--r--  2.0 unx     2878 b- defN 23-Feb-23 11:07 bigdl/nano/utils/pytorch/save.py
+-rw-r--r--  2.0 unx     1189 b- defN 23-Apr-11 11:07 bigdl/nano/utils/pytorch/version.py
+-rw-r--r--  2.0 unx     1012 b- defN 23-Apr-07 11:07 bigdl/nano/utils/pytorch/xpu.py
+-rw-r--r--  2.0 unx     1057 b- defN 23-Mar-09 11:07 bigdl/nano/utils/tf/__init__.py
+-rw-r--r--  2.0 unx     4300 b- defN 23-Feb-19 03:08 bigdl/nano/utils/tf/attributes.py
+-rw-r--r--  2.0 unx     2816 b- defN 23-Apr-04 11:07 bigdl/nano/utils/tf/backend.py
+-rw-r--r--  2.0 unx     7421 b- defN 23-Mar-13 11:07 bigdl/nano/utils/tf/data.py
+-rw-r--r--  2.0 unx     2582 b- defN 23-Mar-09 11:07 bigdl/nano/utils/tf/preprocess.py
+-rw-r--r--  2.0 unx     1347 b- defN 23-Apr-04 11:07 bigdl/nano/utils/tf/subprocess_worker.py
+-rw-r--r--  2.0 unx      823 b- defN 23-Feb-19 03:08 bigdl/nano/utils/tf/version.py
+-rwxr-xr-x  2.0 unx    12068 b- defN 23-Apr-07 11:07 bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init
+-rwxr-xr-x  2.0 unx      909 b- defN 22-Dec-29 11:06 bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init.ps1
+-rwxr-xr-x  2.0 unx      214 b- defN 23-Feb-02 11:06 bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env
+-rwxr-xr-x  2.0 unx      127 b- defN 22-Dec-29 11:06 bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env.ps1
+-rw-r--r--  2.0 unx    10112 b- defN 23-Apr-18 11:09 bigdl_nano-2.3.0b20230418.dist-info/METADATA
+-rw-r--r--  2.0 unx       98 b- defN 23-Apr-18 11:09 bigdl_nano-2.3.0b20230418.dist-info/WHEEL
+-rw-r--r--  2.0 unx       54 b- defN 23-Apr-18 11:09 bigdl_nano-2.3.0b20230418.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        6 b- defN 23-Apr-18 11:09 bigdl_nano-2.3.0b20230418.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    21966 b- defN 23-Apr-18 11:09 bigdl_nano-2.3.0b20230418.dist-info/RECORD
+226 files, 8832020 bytes uncompressed, 2957946 bytes compressed:  66.5%
```

## zipnote {}

```diff
@@ -600,17 +600,23 @@
 
 Filename: bigdl/nano/utils/pytorch/input_sample.py
 Comment: 
 
 Filename: bigdl/nano/utils/pytorch/inspect.py
 Comment: 
 
+Filename: bigdl/nano/utils/pytorch/jit_method.py
+Comment: 
+
 Filename: bigdl/nano/utils/pytorch/load.py
 Comment: 
 
+Filename: bigdl/nano/utils/pytorch/metadata.py
+Comment: 
+
 Filename: bigdl/nano/utils/pytorch/metric.py
 Comment: 
 
 Filename: bigdl/nano/utils/pytorch/model_info.py
 Comment: 
 
 Filename: bigdl/nano/utils/pytorch/save.py
@@ -639,35 +645,35 @@
 
 Filename: bigdl/nano/utils/tf/subprocess_worker.py
 Comment: 
 
 Filename: bigdl/nano/utils/tf/version.py
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init
+Filename: bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init.ps1
+Filename: bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init.ps1
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env
+Filename: bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env.ps1
+Filename: bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env.ps1
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.dist-info/METADATA
+Filename: bigdl_nano-2.3.0b20230418.dist-info/METADATA
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.dist-info/WHEEL
+Filename: bigdl_nano-2.3.0b20230418.dist-info/WHEEL
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.dist-info/entry_points.txt
+Filename: bigdl_nano-2.3.0b20230418.dist-info/entry_points.txt
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.dist-info/top_level.txt
+Filename: bigdl_nano-2.3.0b20230418.dist-info/top_level.txt
 Comment: 
 
-Filename: bigdl_nano-2.3.0b20230417.dist-info/RECORD
+Filename: bigdl_nano-2.3.0b20230418.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bigdl/nano/deps/ipex/ipex_api.py

```diff
@@ -43,15 +43,16 @@
 
     return ret
 
 
 def PytorchIPEXJITModel(model, input_sample=None, use_ipex=False,
                         use_jit=False, channels_last=None, thread_num=None,
                         inplace=False, jit_strict=True, jit_method=None,
-                        weights_prepack=None, enable_onednn=True):
+                        weights_prepack=None, enable_onednn=True,
+                        example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform.
     :param input_sample: torch tensor indicate the data sample to be used
             for tracing.
     :param use_ipex: if use ipex to optimize the model
     :param use_jit: if use jit to accelerate the model
     :param channels_last: if set model and data to be channels-last mode.
@@ -63,27 +64,33 @@
     :param weights_prepack: Whether to perform weight prepack for convolution and linear
            to avoid oneDNN weights reorder. The default value is None. Explicitly setting
            this knob overwrites the configuration set by level knob. Only valid when
            ``use_ipex=True``, otherwise will be ignored.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
            API, which provides a flexible API for aggressive fusion. Default to
            ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
+    :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+           to ``torch.jit.trace``. Default to None. Either this argument or input_sample
+           should be specified when use_jit is ``True`` and torch > 2.0,
+           otherwise will be ignored.
     '''
     from .ipex_inference_model import PytorchIPEXJITModel
     return PytorchIPEXJITModel(model, input_sample=input_sample, use_ipex=use_ipex,
                                use_jit=use_jit, channels_last=channels_last,
                                thread_num=thread_num, inplace=inplace, jit_strict=jit_strict,
                                jit_method=jit_method, weights_prepack=weights_prepack,
-                               enable_onednn=enable_onednn)
+                               enable_onednn=enable_onednn,
+                               example_kwarg_inputs=example_kwarg_inputs)
 
 
 def PytorchIPEXJITBF16Model(model, input_sample=None, use_ipex=False,
                             use_jit=False, channels_last=None, thread_num=None,
                             inplace=False, jit_strict=True, jit_method=None,
-                            weights_prepack=None, enable_onednn=True):
+                            weights_prepack=None, enable_onednn=True,
+                            example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform.
     :param input_sample: torch tensor indicate the data sample to be used
             for tracing.
     :param use_ipex: if use ipex to optimize the model
     :param use_jit: if use jit to accelerate the model
     :param channels_last: if set model and data to be channels-last mode.
@@ -95,45 +102,55 @@
     :param weights_prepack: Whether to perform weight prepack for convolution and linear
            to avoid oneDNN weights reorder. The default value is None. Explicitly setting
            this knob overwrites the configuration set by level knob. Only valid when
            ``use_ipex=True``, otherwise will be ignored.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph
            API, which provides a flexible API for aggressive fusion. Default to
            ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
+    :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+           to ``torch.jit.trace``. Default to None. Either this argument or input_sample
+           should be specified when use_jit is ``True`` and torch > 2.0,
+           otherwise will be ignored.
     '''
     from .ipex_inference_bf16_model import PytorchIPEXJITBF16Model
     return PytorchIPEXJITBF16Model(model, input_sample=input_sample, use_ipex=use_ipex,
                                    use_jit=use_jit, channels_last=channels_last,
                                    thread_num=thread_num, inplace=inplace, jit_strict=jit_strict,
                                    jit_method=jit_method, weights_prepack=weights_prepack,
-                                   enable_onednn=enable_onednn)
+                                   enable_onednn=enable_onednn,
+                                   example_kwarg_inputs=example_kwarg_inputs)
 
 
 def PytorchIPEXQuantizationModel(model, calib_data, q_config=None,
                                  input_sample=None, channels_last=None,
                                  thread_num=None, inplace=False,
-                                 jit_strict=True):
+                                 jit_strict=True, example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform.
     :param calib_data: calibration data is required for static quantization.
     :param q_config: describes how to quantize a layer or a part of the network
             by providing settings (observer classes) for activations and weights
             respectively.
     :param input_sample: torch tensor indicate the data sample to be used
             for tracing.
     :param channels_last: if set model and data to be channels-last mode.
     :param thread_num: the thread num allocated for this model.
     :param inplace: whether to perform inplace optimization. Default: ``False``.
     :param jit_strict: Whether recording your mutable container types.
+    :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+           to ``torch.jit.trace``. Default to None. Either this argument or input_sample
+           should be specified when use_jit is ``True`` and torch > 2.0,
+           otherwise will be ignored.
     '''
     from .ipex_quantization_model import PytorchIPEXQuantizationModel
     return PytorchIPEXQuantizationModel(model, calib_data, q_config=q_config,
                                         input_sample=input_sample, channels_last=channels_last,
                                         thread_num=thread_num, inplace=inplace,
-                                        jit_strict=jit_strict)
+                                        jit_strict=jit_strict,
+                                        example_kwarg_inputs=example_kwarg_inputs)
 
 
 def PytorchIPEXPUModel(model, thread_num=None, precision="fp32", use_ipex=False):
     '''
     :param model: the model(nn.module) to be transform.
     :param thread_num: the thread num allocated for this model.
     '''
```

## bigdl/nano/deps/ipex/ipex_inference_bf16_model.py

```diff
@@ -25,15 +25,15 @@
 
 
 class PytorchIPEXJITBF16Model(PytorchIPEXJITModel):
     def __init__(self, model, input_sample=None, use_ipex=False,
                  use_jit=False, channels_last=None, channels_last_available=[],
                  thread_num=None, from_load=False, inplace=False, jit_strict=True,
                  jit_method=None, weights_prepack=None, enable_onednn=True,
-                 compression="fp32"):
+                 compression="fp32", example_kwarg_inputs=None):
         '''
         This is the accelerated model for pytorch and ipex/jit.
         All the external API is based on InferenceOptimizer, so what we have here is
         basically internal APIs and subject to change.
 
         This PytorchIPEXJITBF16Model will serve for bf16 and ipex>1.9 models.
         :param model: the model(nn.module) to be transform if from_load is False
@@ -59,29 +59,34 @@
                ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
         :param compression: str. This parameter only effective for jit, ipex or pure
                pytorch model with fp32 or bf16 precision. Defaultly, all models are saved
                by dtype=fp32 for their parameters. If users set a lower precision, a smaller
                file sill be saved with some accuracy loss. Users always need to use nano
                to load the compressed file if compression is set other than "fp32".
                Currently, "bf16" and "fp32"(default) are supported.
+        :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+               to ``torch.jit.trace``. Default to ``None``. Either this argument or
+               ``input_sample`` should be specified when ``use_jit`` is ``True`` and
+               torch > 2.0, otherwise will be ignored.
         '''
         if use_ipex:
             invalidInputError(
                 self._check_cpu_isa,
                 errMsg="Applying IPEX BF16 optimization needs the cpu support avx512.",
                 fixMsg="Please set use_ipex to False or not set precision to bf16."
             )
 
         PytorchIPEXJITModel.__init__(self, model, input_sample=input_sample, use_ipex=use_ipex,
                                      dtype=torch.bfloat16, use_jit=use_jit,
                                      channels_last=channels_last,
                                      channels_last_available=channels_last_available,
                                      from_load=from_load, inplace=inplace, jit_strict=jit_strict,
                                      jit_method=jit_method, weights_prepack=weights_prepack,
-                                     enable_onednn=enable_onednn, compression=compression)
+                                     enable_onednn=enable_onednn, compression=compression,
+                                     example_kwarg_inputs=example_kwarg_inputs)
         _accelerator = "jit" if use_jit is True else None
         self._nano_context_manager = generate_context_manager(accelerator=_accelerator,
                                                               precision="bf16",
                                                               thread_num=thread_num,
                                                               enable_onednn=enable_onednn)
 
     @property
```

## bigdl/nano/deps/ipex/ipex_inference_model.py

```diff
@@ -11,31 +11,31 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 from bigdl.nano.utils.pytorch import generate_channels_last_available,\
-    apply_proper_channels_last, transform_state_dict_to_dtype
+    apply_proper_channels_last, transform_state_dict_to_dtype, \
+    patch_attrs_from_model_to_object, jit_convert
 from bigdl.nano.pytorch.model import AcceleratedLightningModule
 from bigdl.nano.pytorch.context_manager import generate_context_manager
 from bigdl.nano.deps.ipex.ipex_api import ipex_optimize
-from bigdl.nano.utils.common import invalidInputError
-from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object
-from bigdl.nano.utils.pytorch import transform_state_dict_to_dtype
+from bigdl.nano.utils.common import invalidInputError, compare_version
+import operator
 import torch
 import copy
 
 
 class PytorchIPEXJITModel(AcceleratedLightningModule):
     def __init__(self, model: torch.nn.Module, input_sample=None, use_ipex=False, dtype=None,
                  use_jit=False, channels_last=None, channels_last_available=[],
                  thread_num=None, from_load=False, inplace=False, jit_strict=True,
                  jit_method=None, weights_prepack=None, enable_onednn=True,
-                 compression="fp32"):
+                 compression="fp32", example_kwarg_inputs=None):
         """
         This is the accelerated model for pytorch and ipex/jit.
         All the external API is based on InferenceOptimizer, so what we have here is
         basically internal APIs and subject to change.
 
         This PytorchIPEXJITModel will serve for fp32 and ipex>1.9 models.
         :param model: the model(nn.module) to be transform if from_load is False
@@ -67,14 +67,18 @@
                ``True``, only valid when use_jit is ``True``, otherwise will be ignored.
         :param compression: str. This parameter only effective for jit, ipex or pure
                pytorch model with fp32 or bf16 precision. Defaultly, all models are saved
                by dtype=fp32 for their parameters. If users set a lower precision, a smaller
                file sill be saved with some accuracy loss. Users always need to use nano
                to load the compressed file if compression is set other than "fp32".
                Currently, "bf16" and "fp32"(default) are supported.
+        :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+               to ``torch.jit.trace``. Default to ``None``. Either this argument or
+               ``input_sample`` should be specified when ``use_jit`` is ``True`` and
+               torch > 2.0, otherwise will be ignored.
         """
         super().__init__(model)
         if from_load:
             self.use_ipex = use_ipex
             self.use_jit = use_jit
             self.channels_last = channels_last
             self.jit_strict = jit_strict
@@ -116,48 +120,22 @@
         else:
             self.channels_last_available = []
         if self.use_ipex:
             self.model = ipex_optimize(self.model, dtype=dtype, inplace=inplace,
                                        weights_prepack=weights_prepack)
 
         if self.use_jit:
-            if dtype == torch.bfloat16:
-                with torch.no_grad():
-                    with torch.cpu.amp.autocast():
-                        if self.jit_method == 'trace':
-                            self.model = torch.jit.trace(self.model, input_sample,
-                                                         check_trace=False,
-                                                         strict=jit_strict)
-                        elif self.jit_method == 'script':
-                            self.model = torch.jit.script(self.model)
-                        else:
-                            try:
-                                self.model = torch.jit.trace(self.model, input_sample,
-                                                             check_trace=False,
-                                                             strict=jit_strict)
-                            except Exception:
-                                self.model = torch.jit.script(self.model)
-                        if self.use_ipex:
-                            self.model = torch.jit.freeze(self.model)
-            else:
-                with torch.no_grad():
-                    if self.jit_method == 'trace':
-                        self.model = torch.jit.trace(self.model, input_sample,
-                                                     check_trace=False,
-                                                     strict=jit_strict)
-                    elif self.jit_method == 'script':
-                        self.model = torch.jit.script(self.model)
-                    else:
-                        try:
-                            self.model = torch.jit.trace(self.model, input_sample,
-                                                         check_trace=False,
-                                                         strict=jit_strict)
-                        except Exception:
-                            self.model = torch.jit.script(self.model)
-                    self.model = torch.jit.freeze(self.model)
+            with torch.no_grad():
+                with torch.cpu.amp.autocast(enabled=dtype == torch.bfloat16):
+                    self.model = jit_convert(self.model, input_sample,
+                                             jit_method=jit_method,
+                                             jit_strict=jit_strict,
+                                             example_kwarg_inputs=example_kwarg_inputs)
+                    if dtype != torch.bfloat16 or self.use_ipex:
+                        self.model = torch.jit.freeze(self.model)
         _accelerator = "jit" if use_jit is True else None
         self._nano_context_manager = generate_context_manager(accelerator=_accelerator,
                                                               precision="fp32",
                                                               thread_num=thread_num,
                                                               enable_onednn=enable_onednn)
         self.thread_num = thread_num
         self.enable_onednn = enable_onednn
```

## bigdl/nano/deps/ipex/ipex_quantization_model.py

```diff
@@ -15,22 +15,25 @@
 #
 
 from collections.abc import Sequence
 import intel_extension_for_pytorch as ipex
 from intel_extension_for_pytorch.quantization import prepare, convert
 from bigdl.nano.pytorch.model import AcceleratedLightningModule
 from bigdl.nano.pytorch.context_manager import generate_context_manager
-from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object
+from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object, jit_convert
+from bigdl.nano.utils.common import compare_version
+import operator
 import torch
 
 
 class PytorchIPEXQuantizationModel(AcceleratedLightningModule):
     def __init__(self, model: torch.nn.Module, calib_data, q_config=None,
                  input_sample=None, channels_last=None, thread_num=None,
-                 from_load=False, inplace=False, jit_strict=True):
+                 from_load=False, inplace=False, jit_strict=True,
+                 example_kwarg_inputs=None):
         """
         This is the accelerated model for pytorch and ipex/jit.
         All the external API is based on InferenceOptimizer, so what we have here is
         basically internal APIs and subject to change.
 
         This PytorchIPEXQuantizationModel will serve for int8 and ipex>1.9 models.
         :param model: the model(nn.module) to be transform if from_load is False
@@ -48,14 +51,18 @@
         :param input_sample: torch tensor indicate the data sample to be used
                for tracing.
         :param channels_last: if set model and data to be channels-last mode.
         :param thread_num: the thread num allocated for this model.
         :param from_load: this will only be set by _load method.
         :param inplace: whether to perform inplace optimization. Default: ``False``.
         :param jit_strict: Whether recording your mutable container types.
+        :param example_kwarg_inputs: keyword arguments of example inputs that will
+               be passed to ``torch.jit.trace``. Default to ``None``. Either this
+               argument or ``input_sample`` should be specified when ``use_jit`` is
+               ``True`` and torch > 2.0, otherwise will be ignored.
         """
         super().__init__(model)
         if from_load:
             self.channels_last = channels_last
             self.jit_strict = jit_strict
             self._nano_context_manager = generate_context_manager(accelerator="jit",
                                                                   precision="int8",
@@ -103,16 +110,18 @@
                 self.model(*x)
             else:
                 self.model(x)
 
         # convert to static quantized model
         self.model = convert(self.model)
         with torch.no_grad():
-            self.model = torch.jit.trace(self.model, input_sample,
-                                         strict=jit_strict)
+            self.model = jit_convert(self.model, input_sample,
+                                     jit_method='trace',
+                                     jit_strict=jit_strict,
+                                     example_kwarg_inputs=example_kwarg_inputs)
             self.model = torch.jit.freeze(self.model)
         # patch attributes from original model
         patch_attrs_from_model_to_object(self.original_model, self)
 
     @property
     def forward_args(self):
         return [input_value.debugName() for input_value in self.model.graph.inputs()
```

## bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py

```diff
@@ -16,31 +16,36 @@
 import torch
 import os
 from pathlib import Path
 from tempfile import TemporaryDirectory
 from ..core.onnxruntime_model import ONNXRuntimeModel
 import onnxruntime  # should be put behind core's import
 from bigdl.nano.pytorch.model import AcceleratedLightningModule
-from bigdl.nano.utils.pytorch import export_to_onnx
+from bigdl.nano.utils.pytorch import export_to_onnx, get_input_example, \
+    get_forward_args
 from bigdl.nano.utils.common import invalidInputError
-from bigdl.nano.pytorch.context_manager import generate_context_manager
-from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object
+from bigdl.nano.pytorch.context_manager import generate_context_manager, BaseContextManager
+from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object, \
+    MetaData
+import pickle
 
 
 class PytorchONNXRuntimeModel(ONNXRuntimeModel, AcceleratedLightningModule):
     '''
         This is the accelerated model for pytorch and onnxruntime.
         All the external API is based on Trainer, so what we have here is
         basically internal APIs and subject to change.
 
         This PytorchONNXRuntimeModel will serve for all precision models.
     '''
 
     def __init__(self, model, input_sample=None, onnxruntime_session_options=None,
-                 simplification=True, dynamic_axes=True, output_tensors=True, **export_kwargs):
+                 simplification=True, dynamic_axes=True, output_tensors=True,
+                 output_metadata=None,
+                 **export_kwargs):
         """
         Create a ONNX Runtime model from pytorch.
 
         :param model: 1. Pytorch model to be converted to ONNXRuntime for inference
                       2. Path to ONNXRuntime saved model.
         :param input_sample: A set of inputs for trace, defaults to None if you have trace before or
                              model is a LightningModule with any dataloader attached,
@@ -61,31 +66,43 @@
                              |
                              | VALUE (dict or list): If a dict, keys are axis indices and values
                              | are axis names. If a list, each element is an axis index.
 
                              If accelerator != 'openvino'/'onnxruntime', it will be ignored.
         :param output_tensors: boolean, default to True and output of the model will be Tensors.
                                If output_tensors=False, output of the ONNX model will be ndarray.
+        :param output_metadata: metadata of model output, defaults to None.
         :param **export_kwargs: will be passed to torch.onnx.export function.
         """
         # Typically, when model is int8, we use this path
         # TODO: self._forward_args should be set externally
+        self.output_metadata = output_metadata
         with TemporaryDirectory() as tmpdir:
             if isinstance(model, torch.nn.Module):
                 onnx_path = os.path.join(tmpdir, "tmp.onnx")
                 # Typically, when model is fp32, we use this path
                 export_to_onnx(model, input_sample=input_sample, onnx_path=onnx_path,
                                dynamic_axes=dynamic_axes, **export_kwargs)
                 if simplification is True:
                     # simplify model
                     try:
                         from bigdl.nano.deps.onnxsim.onnxsim_api import onnx_simplify
                         onnx_simplify(onnx_path)
                     except Exception:
                         pass
+
+                # test run to get output metadata
+                with BaseContextManager():
+                    forward_args = get_forward_args(model)
+                    input_sample = get_input_example(model, input_sample, forward_args)
+                    if isinstance(input_sample, (tuple, list)):
+                        output = model(*input_sample)
+                    else:
+                        output = model(input_sample)
+                    self.output_metadata = MetaData.construct_matadata(output)
             else:
                 onnx_path = model
             AcceleratedLightningModule.__init__(self, None)
             ONNXRuntimeModel.__init__(self, onnx_path, session_options=onnxruntime_session_options)
         if onnxruntime_session_options.intra_op_num_threads > 0:
             self.thread_num = onnxruntime_session_options.intra_op_num_threads
         else:
@@ -110,22 +127,26 @@
         return kwargs
 
     def on_forward_end(self, outputs):
         if self.output_tensors:
             outputs = self.numpy_to_tensors(outputs)
         elif len(outputs) == 1:
             outputs = outputs[0]
+        if self.output_metadata is not None:
+            outputs = MetaData.reconstruct_output(outputs, self.output_metadata)
         return outputs
 
     @property
     def status(self):
         status = super().status
         status.update({"onnx_path": 'onnx_saved_model.onnx',
+                       "metadata_path": 'matadata.pkl',
                        "intra_op_num_threads": self.session_options.intra_op_num_threads,
-                       "inter_op_num_threads": self.session_options.inter_op_num_threads})
+                       "inter_op_num_threads": self.session_options.inter_op_num_threads,
+                       "output_tensors": self.output_tensors})
         return status
 
     @staticmethod
     def _load(path):
         """
         Load an ONNX model for inference from directory.
 
@@ -144,13 +165,26 @@
         onnxruntime_session_options = onnxruntime.SessionOptions()
         if status.get('intra_op_num_threads', None):
             onnxruntime_session_options.intra_op_num_threads = \
                 status.get('intra_op_num_threads', None)
         if status.get('inter_op_num_threads', None):
             onnxruntime_session_options.inter_op_num_threads = \
                 status.get('inter_op_num_threads', None)
+        output_tensors = status.get('output_tensors', True)
+        # load meatdata
+        metadata_path = status.get('metadata_path', None)
+        if metadata_path is None or not metadata_path:
+            output_metadata = None
+        else:
+            with open(path / status['metadata_path'], "rb") as f:
+                output_metadata = pickle.load(f)
         return PytorchONNXRuntimeModel(str(onnx_path),
-                                       onnxruntime_session_options=onnxruntime_session_options)
+                                       onnxruntime_session_options=onnxruntime_session_options,
+                                       output_tensors=output_tensors,
+                                       output_metadata=output_metadata)
 
     def _save_model(self, path, compression="fp32"):
         onnx_path = Path(path) / self.status['onnx_path']
         super()._save_model(onnx_path)
+        # save metadata
+        with open(path / self.status['metadata_path'], "wb") as f:
+            pickle.dump(self.output_metadata, f)
```

## bigdl/nano/deps/openvino/pytorch/model.py

```diff
@@ -21,23 +21,25 @@
 from ..core.model import OpenVINOModel
 from bigdl.nano.pytorch.model import AcceleratedLightningModule
 from .utils import export
 import torch
 from bigdl.nano.utils.common import invalidInputError
 from ..core.utils import save
 from torch.utils.data.dataloader import DataLoader
-from bigdl.nano.pytorch.context_manager import generate_context_manager
-from bigdl.nano.utils.pytorch import patch_attrs_from_model_to_object
+from bigdl.nano.pytorch.context_manager import generate_context_manager, BaseContextManager
+from bigdl.nano.utils.pytorch import get_input_example, get_forward_args, \
+    patch_attrs_from_model_to_object, MetaData
+import pickle
 
 
 class PytorchOpenVINOModel(AcceleratedLightningModule):
     def __init__(self, model, input_sample=None, precision='fp32',
                  thread_num=None, device='CPU', dynamic_axes=True,
                  logging=True, config=None, output_tensors=True,
-                 shapes=None, **kwargs):
+                 shapes=None, output_metadata=None, **kwargs):
         """
         Create a OpenVINO model from pytorch.
 
         :param model: Pytorch model to be converted to OpenVINO for inference or
                       path to Openvino saved model.
         :param input_sample: A set of inputs for trace, defaults to None if you have trace before or
                              model is a LightningModule with any dataloader attached,
@@ -66,16 +68,18 @@
         :param config: The config to be inputted in core.compile_model.
         :param output_tensors: boolean, default to True and output of the model will be Tensors. If
                                output_tensors=False, output of the OpenVINO model will be ndarray.
         :param shapes: input shape. For example, 'input1[1,3,224,224],input2[1,4]',
                        '[1,3,224,224]'. This parameter affect model Parameter shape, can be
                        dynamic. For dynamic dimesions use symbol `?`, `-1` or range `low.. up`.'.
                        Only valid for openvino model, otherwise will be ignored.
+        :param output_metadata: metadata of model output, defaults to None.
         :param **kwargs: will be passed to torch.onnx.export function or model optimizer function.
         """
+        self.output_metadata = output_metadata
         ov_model_path = model
         with TemporaryDirectory() as tmpdir:
             tmpdir = Path(tmpdir)
             if isinstance(model, torch.nn.Module):
                 # cope with dynamic axes for GPU
                 if device != 'CPU':
                     if dynamic_axes is True or (
@@ -87,14 +91,24 @@
                                           "optimizer. For more details about model optimizer, you "
                                           "can see mo --help .")
                 export(model, input_sample, str(tmpdir / 'tmp.xml'),
                        precision=precision, dynamic_axes=dynamic_axes,
                        logging=logging, **kwargs)
                 ov_model_path = tmpdir / 'tmp.xml'
 
+                # test run to get output metadata
+                with BaseContextManager():
+                    forward_args = get_forward_args(model)
+                    input_sample = get_input_example(model, input_sample, forward_args)
+                    if isinstance(input_sample, (list, tuple)):
+                        output = model(*input_sample)
+                    else:
+                        output = model(input_sample)
+                    self.output_metadata = MetaData.construct_matadata(output)
+
             self.ov_model = OpenVINOModel(ov_model_path,
                                           device=device,
                                           precision=precision,
                                           thread_num=thread_num,
                                           config=config,
                                           shapes=shapes)
             super().__init__(None)
@@ -120,26 +134,31 @@
 
     def on_forward_end(self, outputs):
         outputs = list(outputs.values())
         if self.output_tensors:
             outputs = self.numpy_to_tensors(outputs)
         elif len(outputs) == 1:
             outputs = outputs[0]
+        if self.output_metadata is not None:
+            outputs = MetaData.reconstruct_output(outputs, self.output_metadata)
         return outputs
 
     def reshape(self, shapes):
         return self.ov_model.reshape(shapes=shapes)
 
     @property
     def status(self):
         status = super().status
         status.update({"xml_path": 'ov_saved_model.xml',
+                       "metadata_path": 'matadata.pkl',
                        "weight_path": 'ov_saved_model.bin',
                        "config": self.ov_model.final_config,
-                       "device": self.ov_model._device})
+                       "device": self.ov_model._device,
+                       "output_tensors": self.output_tensors,
+                       })
         return status
 
     @property  # type: ignore
     def forward_args(self):
         return self.ov_model.forward_args
 
     @staticmethod
@@ -165,19 +184,29 @@
             thread_num = int(config["CPU_THREADS_NUM"])
         elif "INFERENCE_NUM_THREADS" in config:
             thread_num = int(config["INFERENCE_NUM_THREADS"])
         if cache_dir is not None:
             config["CACHE_DIR"] = cache_dir
         if device is None:
             device = status.get('device', 'CPU')
+        output_tensors = status.get('output_tensors', True)
+        # load meatdata
+        metadata_path = status.get('metadata_path', None)
+        if metadata_path is None or not metadata_path:
+            output_metadata = None
+        else:
+            with open(path / status['metadata_path'], "rb") as f:
+                output_metadata = pickle.load(f)
         return PytorchOpenVINOModel(xml_path,
                                     config=config,
                                     thread_num=thread_num,
                                     device=device,
-                                    shapes=shapes)
+                                    shapes=shapes,
+                                    output_tensors=output_tensors,
+                                    output_metadata=output_metadata)
 
     def pot(self,
             dataloader,
             metric=None,
             higher_better=True,
             drop_type="relative",
             maximal_drop=0.999,
@@ -211,14 +240,17 @@
         :param path: Directory to save the model.
         """
         self.ov_model._model_exists_or_err()
         path = Path(path)
         path.mkdir(exist_ok=True)
         xml_path = path / self.status['xml_path']
         save(self.ov_model.ie_network, xml_path)
+        # save metadata
+        with open(path / self.status['metadata_path'], "wb") as f:
+            pickle.dump(self.output_metadata, f)
 
     def async_predict(self,
                       input_data: Union[DataLoader, List[torch.Tensor], List[List[torch.Tensor]]],
                       num_requests: int = 0) -> List[torch.Tensor]:
         """
         Perfrom model inference using async mode.
```

## bigdl/nano/pytorch/inference/optimizer.py

```diff
@@ -590,14 +590,15 @@
                  sample_size: int = 100,
                  logging: bool = True,
                  inplace: bool = False,
                  weights_prepack: Optional[bool] = None,
                  enable_onednn: bool = True,
                  q_config=None,
                  output_tensors: bool = True,
+                 example_kwarg_inputs=None,
                  **kwargs):
         """
         Calibrate a torch.nn.Module for post-training quantization.
 
         :param model:           A model to be quantized. Model type should be an instance of
                                 torch.nn.Module.
         :param precision:       Global precision of quantized model,
@@ -741,14 +742,20 @@
                          collection of quantization configurations, user can set the qconfig for
                          each operator (torch op calls, functional calls, module calls) in the
                          model through qconfig_mapping.
         :param output_tensors: boolean, default to True and output of the model will be Tensors,
                                only valid when accelerator='onnxruntime' or accelerator='openvino',
                                otherwise will be ignored. If output_tensors=False, output of the
                                export model will be ndarray.
+        :param example_kwarg_inputs: a pack of keyword arguments of example inputs that will be
+                                     passed to ``torch.jit.trace``. Default: ``None``. Either
+                                     this argument or ``input_sample`` should be specified. The
+                                     dict will be unpacking by the arguments name of the traced
+                                     function. Only valid when accelerator='jit' and torch>=2.0,
+                                     otherwise will be ignored.
         :param **kwargs: Other extra advanced settings include:
                          1. those be passed to ``torch.onnx.export`` function,
                          only valid when accelerator='onnxruntime'/'openvino',
                          otherwise will be ignored.
                          Possible arguments are: input_names, output_names, opset_version,
                          et al. For more details, please refer
                          https://pytorch.org/docs/stable/onnx.html#torch.onnx.export.
@@ -790,15 +797,16 @@
                     return PytorchIPEXJITBF16Model(model, input_sample=input_sample,
                                                    use_ipex=use_ipex, use_jit=use_jit,
                                                    channels_last=channels_last,
                                                    thread_num=thread_num, inplace=inplace,
                                                    jit_strict=jit_strict,
                                                    jit_method=jit_method,
                                                    weights_prepack=weights_prepack,
-                                                   enable_onednn=enable_onednn)
+                                                   enable_onednn=enable_onednn,
+                                                   example_kwarg_inputs=example_kwarg_inputs)
                 else:
                     bf16_model = BF16Model(model, channels_last=channels_last,
                                            input_sample=input_sample,
                                            thread_num=thread_num)
                     return bf16_model
             elif accelerator == "openvino":
                 invalidInputError(device == 'CPU',
@@ -969,15 +977,16 @@
                                            calib_dataloader,
                                            q_config=q_config,
                                            input_sample=input_sample,
                                            channels_last=channels_last,
                                            thread_num=thread_num,
                                            jit_strict=jit_strict,
                                            jit_method=jit_method,
-                                           enable_onednn=enable_onednn)
+                                           enable_onednn=enable_onednn,
+                                           example_kwarg_inputs=example_kwarg_inputs)
             else:
                 invalidInputError(False,
                                   "Accelerator {} is invalid.".format(accelerator))
         if precision == 'fp16':
             invalidInputError(accelerator in ['openvino', None],
                               "fp16 is not supported on {} accelerator.".format(accelerator))
             if device == 'VPUX':
@@ -1019,14 +1028,15 @@
               dynamic_axes: Union[bool, dict] = True,
               logging: bool = True,
               inplace: bool = False,
               weights_prepack: Optional[bool] = None,
               enable_onednn: bool = True,
               output_tensors: bool = True,
               strict_check: bool = True,
+              example_kwarg_inputs=None,
               **kwargs):
         """
         Trace a torch.nn.Module and convert it into an accelerated module for inference.
 
         For example, this function returns a PytorchOpenVINOModel when accelerator=='openvino'.
 
         :param model: A torch.nn.Module model, including pl.LightningModule.
@@ -1089,23 +1099,29 @@
                                 parameter to ``False``.
         :param enable_onednn: Whether to use PyTorch JIT graph fuser based on oneDNN Graph API,
                               which provides a flexible API for aggressive fusion. Default to
                               ``True``, only valid when accelerator='jit', otherwise will be
                               ignored. For more details, please refer https://github.com/pytorch/
                               pytorch/tree/master/torch/csrc/jit/codegen/
                               onednn#pytorch---onednn-graph-api-bridge.
-        :param output_tensors: boolean, default to True and output of the model will be Tensors,
+        :param output_tensors: boolean, default to ``True`` and output of the model will be Tensors,
                                only valid when accelerator='onnxruntime' or accelerator='openvino',
                                otherwise will be ignored. If output_tensors=False, output of the
                                export model will be ndarray.
         :param strict_check: some checking in ``trace`` is non-trivial while not critical for the
                              optimization (e.g., if the model is a nn.Module or its subclass). This
                              param helps to eliminate the not critical checking, which may enable
                              more models to be optimized while may bring some strange error
-                             message. Default to True.
+                             message. Default to ``True``.
+        :param example_kwarg_inputs: a pack of keyword arguments of example inputs that will be
+                                     passed to ``torch.jit.trace``. Default: ``None``. Either
+                                     this argument or ``input_sample`` should be specified. The
+                                     dict will be unpacking by the arguments name of the traced
+                                     function. Only valid when accelerator='jit' and torch>=2.0,
+                                     otherwise will be ignored.
         :param **kwargs: Other extra advanced settings include:
                          1. those be passed to torch.onnx.export function,
                          only valid when accelerator='onnxruntime'/'openvino',
                          otherwise will be ignored.
                          Possible arguments are: input_names, output_names, opset_version,
                          et al. For more details, please refer
                          https://pytorch.org/docs/stable/onnx.html#torch.onnx.export.
@@ -1163,25 +1179,26 @@
                                            output_tensors=output_tensors,
                                            **kwargs)
         if accelerator is None and device == "GPU":
             return PytorchIPEXPUModel(model, thread_num=thread_num, use_ipex=use_ipex).to("xpu")
         if (accelerator == 'jit' or use_ipex is True or channels_last is True) and device == "CPU":
             if use_ipex:
                 invalidInputError(not TORCH_VERSION_LESS_1_10,
-                                  "torch version should >=1.10 to use ipex")
+                                  "torch version should >= 1.10 to use ipex")
             use_jit = (accelerator == "jit")
             if use_jit:
                 invalidInputError(jit_method in [None, 'trace', 'script'],
                                   "jit_method {} is invalid.".format(jit_method))
             return PytorchIPEXJITModel(model, input_sample=input_sample, use_ipex=use_ipex,
                                        use_jit=use_jit, channels_last=channels_last,
                                        thread_num=thread_num, inplace=inplace,
                                        jit_strict=jit_strict, jit_method=jit_method,
                                        weights_prepack=weights_prepack,
-                                       enable_onednn=enable_onednn)
+                                       enable_onednn=enable_onednn,
+                                       example_kwarg_inputs=example_kwarg_inputs)
         invalidInputError(False, "Accelerator {} is invalid.".format(accelerator))
 
     @staticmethod
     def get_context(model: nn.Module, *models):
         """
         Obtain corresponding context manager from (multi) model, defaults to BaseContextManager().
```

## bigdl/nano/pytorch/low_precision/jit_int8_api.py

```diff
@@ -14,15 +14,16 @@
 # limitations under the License.
 #
 
 
 def PytorchJITINT8Model(model, calib_data, q_config=None,
                         input_sample=None, channels_last=False,
                         thread_num=None, jit_strict=True,
-                        jit_method=None, enable_onednn=False):
+                        jit_method=None, enable_onednn=False,
+                        example_kwarg_inputs=None):
     '''
     :param model: the model(nn.module) to be transform if from_load is False
            the accelerated model if from_load is True.
     :param calib_data: calibration data is required for static quantization.
     :param q_config: We support 2 types of input here:
 
            | 1. Qconfig (https://pytorch.org/docs/stable/generated/torch.quantization.
@@ -43,18 +44,23 @@
     :param from_load: this will only be set by _load method.
     :param jit_strict: Whether recording your mutable container types.
     :param jit_method: use ``jit.trace`` or ``jit.script`` to convert a model
            to TorchScript.
     :param enable_onednn: Whether to use PyTorch JIT graph fuser based on
            oneDNN Graph API, which provides a flexible API for aggressive
            fusion. Default to ``False``.
+    :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+           to ``torch.jit.trace``. Default to None. Either this argument or input_sample
+           should be specified when use_jit is ``True`` and torch > 2.0,
+           sotherwise will be ignored.
     '''
     from .jit_int8_model import PytorchJITINT8Model
     return PytorchJITINT8Model(model, calib_data, q_config=q_config,
                                input_sample=input_sample, channels_last=channels_last,
                                thread_num=thread_num, jit_strict=jit_strict,
-                               jit_method=jit_method, enable_onednn=enable_onednn)
+                               jit_method=jit_method, enable_onednn=enable_onednn,
+                               example_kwarg_inputs=example_kwarg_inputs)
 
 
 def load_pytorchjitint8_model(path):
     from .jit_int8_model import PytorchJITINT8Model
     return PytorchJITINT8Model._load(path)
```

## bigdl/nano/pytorch/low_precision/jit_int8_model.py

```diff
@@ -13,27 +13,27 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 
 from bigdl.nano.pytorch.context_manager import generate_context_manager
 from bigdl.nano.pytorch.model import AcceleratedLightningModule
-
+from bigdl.nano.utils.common import compare_version
 import torch
 from torch.ao.quantization import QConfig, get_default_qconfig_mapping
 from torch.quantization.quantize_fx import prepare_fx, convert_fx
-
+import operator
 from collections.abc import Sequence
 
 
 class PytorchJITINT8Model(AcceleratedLightningModule):
     def __init__(self, model: torch.nn.Module, calib_data, q_config=None,
                  input_sample=None, channels_last=False, thread_num=None,
                  from_load=False, jit_strict=True, jit_method=None,
-                 enable_onednn=False):
+                 enable_onednn=False, example_kwarg_inputs=None):
         '''
         This is the accelerated model for pytorch fx quantization and jit.
         All the external API is based on InferenceOptimizer, so what we have here is
         basically internal APIs and subject to change.
 
         :param model: the model(nn.module) to be transform if from_load is False
                the accelerated model if from_load is True.
@@ -58,14 +58,18 @@
         :param from_load: this will only be set by _load method.
         :param jit_strict: Whether recording your mutable container types.
         :param jit_method: use ``jit.trace`` or ``jit.script`` to convert a model
                to TorchScript.
         :param enable_onednn: Whether to use PyTorch JIT graph fuser based on
                oneDNN Graph API, which provides a flexible API for aggressive
                fusion. Default to ``False``.
+        :param example_kwarg_inputs: keyword arguments of example inputs that will be passed
+               to ``torch.jit.trace``. Default to None. Either this argument or input_sample
+               should be specified when use_jit is ``True`` and torch > 2.0,
+               otherwise will be ignored.
         '''
         super().__init__(model)
 
         enable_onednn = False
         # TODO: since onednn cooperates well with other nano methods, it is set to True
         # by default in InferenceOptimizer.quantize(). However, it will lead to strange
         # error in fx quantization during inference. Therefore, we disable it by hand.
@@ -136,22 +140,44 @@
             else:
                 self.model(x)
 
         self.model = convert_fx(self.model)
 
         with torch.no_grad():
             if self.jit_method == 'trace':
-                self.model = torch.jit.trace(self.model, input_sample,
-                                             strict=jit_strict)
+                if compare_version("torch", operator.ge, "2.0"):
+                    if example_kwarg_inputs is not None:
+                        input_sample = None
+                    self.model = torch.jit.trace(
+                        self.model,
+                        example_inputs=input_sample,
+                        check_trace=False,
+                        strict=jit_strict,
+                        example_kwarg_inputs=example_kwarg_inputs)
+                else:
+                    self.model = torch.jit.trace(
+                        self.model, input_sample,
+                        check_trace=False,
+                        strict=jit_strict)
             elif self.jit_method == 'script':
                 self.model = torch.jit.script(self.model)
             else:
                 try:
-                    self.model = torch.jit.trace(self.model, input_sample,
-                                                 strict=jit_strict)
+                    if compare_version("torch", operator.ge, "2.0"):
+                        self.model = torch.jit.trace(
+                            self.model,
+                            example_inputs=input_sample,
+                            check_trace=False,
+                            strict=jit_strict,
+                            example_kwarg_inputs=example_kwarg_inputs)
+                    else:
+                        self.model = torch.jit.trace(
+                            self.model, input_sample,
+                            check_trace=False,
+                            strict=jit_strict)
                 except Exception:
                     self.model = torch.jit.script(self.model)
             self.model = torch.jit.freeze(self.model)
 
     def on_forward_start(self, inputs):
         return inputs
```

## bigdl/nano/utils/pytorch/__init__.py

```diff
@@ -50,7 +50,11 @@
 from .convert import export_to_onnx
 
 from .save import save_model
 from .save import transform_state_dict_to_dtype
 from .load import load_model
 
 from .xpu import apply_data_to_xpu, apply_data_to_half
+
+from .jit_method import jit_convert
+
+from .metadata import MetaData
```

## Comparing `bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init` & `bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init`

 * *Files identical despite different names*

## Comparing `bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init.ps1` & `bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init.ps1`

 * *Files identical despite different names*

## Comparing `bigdl_nano-2.3.0b20230417.dist-info/METADATA` & `bigdl_nano-2.3.0b20230418.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: bigdl-nano
-Version: 2.3.0b20230417
+Version: 2.3.0b20230418
 Summary: High-performance scalable acceleration components for intel.
 Home-page: https://github.com/intel-analytics/BigDL
 Author: BigDL Authors
 Author-email: bigdl-user-group@googlegroups.com
 License: UNKNOWN
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
```

## Comparing `bigdl_nano-2.3.0b20230417.dist-info/RECORD` & `bigdl_nano-2.3.0b20230418.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -32,19 +32,19 @@
 bigdl/nano/deps/automl/optuna_backend.py,sha256=uiYeGc4GXv4bUiZxt69UqjuZPp3nsYwghiDm77Ulm8w,6876
 bigdl/nano/deps/horovod/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/horovod/distributed_utils_horovod.py,sha256=VIwjFWYc3km0Tz1Mq5cLuJuqFzgvhzUcBPA-mkRgZjg,5304
 bigdl/nano/deps/horovod/horovod_api.py,sha256=JTTEzkhGU6UpKitUrLXJyd3e_9rgItn-dh9i1o8eYb8,934
 bigdl/nano/deps/horovod/horovod_worker.py,sha256=StZxIukKUbvW1aKn-c6WAzVHkFZLiyplgyyDWq8GJGA,1125
 bigdl/nano/deps/horovod/multiprocs_backend.py,sha256=KBr8aXLO1tzwCnTdM4dab0He-oWMoK_w3H2IB_qeBT8,2423
 bigdl/nano/deps/ipex/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
-bigdl/nano/deps/ipex/ipex_api.py,sha256=vGDiNV9Bg-IqJEQF7rIjXvbWO1Pw5L7wnI4-XeowyaY,8275
-bigdl/nano/deps/ipex/ipex_inference_bf16_model.py,sha256=9TE6T83plRwu2oDthU_m1gdG8lMhDGfoxzRVDrBE0Y4,8121
-bigdl/nano/deps/ipex/ipex_inference_model.py,sha256=tgHq5Q5ShAAXfapJP2njCZggPMbPTppUPG1I8MK8vJU,15262
+bigdl/nano/deps/ipex/ipex_api.py,sha256=oR8uIcoolORKtpaeLponC0Et3I2j9PJ_zYA-96ZmO0Q,9510
+bigdl/nano/deps/ipex/ipex_inference_bf16_model.py,sha256=dftb484lCmON7bXgY4JGkhM-Pd9DkiPUgAlJ9JnSASo,8545
+bigdl/nano/deps/ipex/ipex_inference_model.py,sha256=2g-M7pE2trpoxH-QBMBc6WJMFjgD3FYjj_1wsoYGyx4,14090
 bigdl/nano/deps/ipex/ipex_inference_xpu_model.py,sha256=kI0-5pCtOKxE-9moZvEv_ZX75ZrOus7hsoLSKc5LKEU,4444
-bigdl/nano/deps/ipex/ipex_quantization_model.py,sha256=zFGqqnja4_eXHZr8FPFKoQCsMgcglQajQVmulDH-jH0,7512
+bigdl/nano/deps/ipex/ipex_quantization_model.py,sha256=9nd-r8bAx1Vdhym1NpBfBHpBOmW8w2ngjjNit2rlUI0,8087
 bigdl/nano/deps/neural_compressor/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/neural_compressor/inc_api.py,sha256=dV79vfNV4oxXZxlJbTsEojZanuO6ji296xTpCm5UaP8,3907
 bigdl/nano/deps/neural_compressor/inc_api_2.py,sha256=Rs5XGKuX2rF_vo0tMxIFyDTf_4AYEBB-NVVHxwJpXWg,9975
 bigdl/nano/deps/neural_compressor/core/__init__.py,sha256=x-EqdMB3cq0-KGFPSfUq2CpFzBYoGbNFSBXKfoRkc4w,1013
 bigdl/nano/deps/neural_compressor/core/base_metric.py,sha256=qaMc4sZzffzBvagXHgOMWxLCDY0PaABKRZkBnc32kAQ,1856
 bigdl/nano/deps/neural_compressor/core/quantization.py,sha256=sNqYHv7iL0LJc0CMcA4wsVWVQQvM425UrUui1gfSfok,10631
 bigdl/nano/deps/neural_compressor/onnx/__init__.py,sha256=OTLYPpPF1-_8Dx1CENZVd_e6HG2ypQnee6P2fgMs7lU,1036
@@ -67,29 +67,29 @@
 bigdl/nano/deps/neural_compressor/tensorflow/quantization.py,sha256=R8QB0iZ5QTWK21qj6ig4JVPPkF_QPO-eQoQhRZRFRxU,3007
 bigdl/nano/deps/neural_compressor/tensorflow/utils.py,sha256=lqMDYKCe2pa2gv1nrAl4jEafmH-Ulhguq0mue_c35e4,729
 bigdl/nano/deps/onnxruntime/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/onnxruntime/onnxruntime_api.py,sha256=E5zNSDBg8dV6-b-4iRjT4xLzxjLFla0ofOe990fpVy0,4990
 bigdl/nano/deps/onnxruntime/core/__init__.py,sha256=9o3a7HDOY3zh5Ow2ggOMD3A2vmZXHOT6ZvZXCjKktBI,932
 bigdl/nano/deps/onnxruntime/core/onnxruntime_model.py,sha256=EO7OaXambGRpkavx_CjixBWqzUn8w-mPn5Gu6yOMihw,3655
 bigdl/nano/deps/onnxruntime/pytorch/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
-bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py,sha256=PHoQMZsvg-kPlXPnBYDoLxk55WhHh3yBrNpWGTka_0o,7854
+bigdl/nano/deps/onnxruntime/pytorch/pytorch_onnxruntime_model.py,sha256=5NIUnmTmeTdvAkXgua4lvIUpaKt5VW_bKOD1wMKT7Cs,9530
 bigdl/nano/deps/onnxruntime/tensorflow/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/onnxruntime/tensorflow/model.py,sha256=Cv0C3B18T9Sj1GWnHgnxOIU9_YhudrTwYZsbAuyXZH8,7852
 bigdl/nano/deps/onnxsim/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/onnxsim/onnxsim_api.py,sha256=wywKDXPEO7bi_MY12UkoncEADvnhztqoqyJp8xo5ubc,1105
 bigdl/nano/deps/openvino/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/openvino/openvino_api.py,sha256=n53VlK4kcDdwS_HYcV4GAoXjvooxDqeHGNsBqHZ6cqk,6986
 bigdl/nano/deps/openvino/core/__init__.py,sha256=kSQ0UT3QO1LNOONyFMu6MjG7MVx_Cf_131OBy2e_jhw,872
 bigdl/nano/deps/openvino/core/metric.py,sha256=KCEAgwrKm_QFNrQvM51lsKZt7k1njT-jtyJlqgTEZ6I,2129
 bigdl/nano/deps/openvino/core/model.py,sha256=rfBTcF7cP5d6pOJg_9x71XYvg5oNLBku2c0VsSf04-s,14225
 bigdl/nano/deps/openvino/core/utils.py,sha256=kelcmlkhHp8m3NYme6ZD5YCgL9i1iguR2tphy2eCv8U,4075
 bigdl/nano/deps/openvino/pytorch/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/openvino/pytorch/dataloader.py,sha256=7DNz7QvQSySrVwowGizzyNfJqXOXoh1UsL0VOiM8yR0,1519
 bigdl/nano/deps/openvino/pytorch/metric.py,sha256=hM3nhIeeK0rOABk3s3mzQa3q4xrqeVX_CK0Aq2be5Gw,1668
-bigdl/nano/deps/openvino/pytorch/model.py,sha256=RzX8sDT8Yx1AJFnXBZjoVXlW0BwO2YLz0j4FVaUAJic,13001
+bigdl/nano/deps/openvino/pytorch/model.py,sha256=mBx2Pm2iJBEYh-qM7IoxzGnh0627M4JZj0NdXJ-VxHE,14656
 bigdl/nano/deps/openvino/pytorch/utils.py,sha256=B_bX2uGKVwNaJEY6i55tR7j2vI0yHsD75bw0HLxA_lk,2616
 bigdl/nano/deps/openvino/tf/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
 bigdl/nano/deps/openvino/tf/dataloader.py,sha256=G9Ti71qinxqqMjO7BdhBHe8kBSKydIBd54PfzTa0IqI,1845
 bigdl/nano/deps/openvino/tf/metric.py,sha256=ey7xQUSOQtAv5k_45JB4FUrKWbckaomxmqUx5OwkmuM,1499
 bigdl/nano/deps/openvino/tf/model.py,sha256=OAB1QKT0sC-aKNuAC-4-5mDBGhT_xxi_Xajr-TOJb1Q,10317
 bigdl/nano/deps/openvino/tf/utils.py,sha256=AqtLgumFpX4LHYEACWrw4ZLi8-Pujmjj2F93efzeSnw,1676
 bigdl/nano/deps/ray/__init__.py,sha256=b2IXvVqQ5cItki021h8s3ymW12RPu8QNPprq4Mn3bDM,586
@@ -114,18 +114,18 @@
 bigdl/nano/pytorch/amp/__init__.py,sha256=5T_pNFHbS-EdK7nlbJJqb4JlCvUnPQpO-sxVZuaP6Ys,609
 bigdl/nano/pytorch/amp/amp_api.py,sha256=RSZda43RuQZJB3sfvMR58WIHrj_RyHQHn3NOc0O_HSQ,805
 bigdl/nano/pytorch/amp/bfloat16.py,sha256=Qw9ZXUxDeIgBFfPNedS9PwSi7FPl1bG-9HrloKyz_lw,9569
 bigdl/nano/pytorch/encryption/__init__.py,sha256=4e0eEjjj3iNwnCRzLtzVjS27oH-CA1ws6TJXnuUwNg8,640
 bigdl/nano/pytorch/encryption/encryption.py,sha256=UIiNtbyZ0GFlkirOHX2hOzxKXtYJ6g56SQ2lJvO40kY,6480
 bigdl/nano/pytorch/inference/__init__.py,sha256=PlFS2XblVRBXTai86gEV79sQWEJLgaOyDnjdLYNKxTA,661
 bigdl/nano/pytorch/inference/multi_instance.py,sha256=XCMQRg1BHfuQWxNTtsHpplZfSNSj0rEO71if67OsR-I,4231
-bigdl/nano/pytorch/inference/optimizer.py,sha256=stRj_ra-FZPTugYuP4B91JLmSenLytiV1n2NPlCeYXU,86860
+bigdl/nano/pytorch/inference/optimizer.py,sha256=XWxR_Vt39v40kCRqachrZf6Qc1SY5Op2y9l5OW1toSs,88302
 bigdl/nano/pytorch/inference/pipeline.py,sha256=lpU1OyoIjYsVf6nw0IF8JH82Z228_WSMj_qZtww1F6A,4593
-bigdl/nano/pytorch/low_precision/jit_int8_api.py,sha256=wPcuXWKUmhBc45cft7PvRFQjM_rRRzhDK0LKTm2zYPc,3015
-bigdl/nano/pytorch/low_precision/jit_int8_model.py,sha256=XKY0Js0ZeXurOt6Kcl--FHX4zyzbzC5XTAcB0hooVK8,9651
+bigdl/nano/pytorch/low_precision/jit_int8_api.py,sha256=HJeNprNAa6DyCx7PLZvM26j63kEtr1luo-hjVJK-ypk,3430
+bigdl/nano/pytorch/low_precision/jit_int8_model.py,sha256=pjYvB_5Wt4CEFXr5tPyo2WIE7tpKcv7BFQ1JqEoHYXU,11050
 bigdl/nano/pytorch/optim/__init__.py,sha256=JWc3YVybOcyixkTxAB2QVN9l4KZC7BXg8D2myUs2P-w,646
 bigdl/nano/pytorch/optim/sparseadam.py,sha256=22GGm0Kh2idtAgEj5mn5asMYc3CoKbbE-gwPqtXwrgA,8351
 bigdl/nano/pytorch/patching/__init__.py,sha256=LGg-BpIbWHdj_LNe4Px1m2lU-Y9LFAmWtctRF9wbCFM,804
 bigdl/nano/pytorch/patching/dtype_patching/__init__.py,sha256=P99b-m_UygHUv-c1jsfE2Q0dtGkbM_mt_GFio7bBFlQ,627
 bigdl/nano/pytorch/patching/dtype_patching/dtype_patching.py,sha256=AL054yiXtljxh9GrqNRRL1wqzPzacQV8LnyEiXOZH-c,4206
 bigdl/nano/pytorch/patching/encryption_patching/__init__.py,sha256=G8mkHd00JeSdgamd8QTemg4ZPx59VDN7N6HgbnMLxbM,637
 bigdl/nano/pytorch/patching/encryption_patching/encryption_patching.py,sha256=Ldygx4lor-gff-Cbzb05YGItBSOdCv1lNytAnqHqX0U,1854
@@ -187,38 +187,40 @@
 bigdl/nano/utils/common/version.py,sha256=oInvdxn0UIQHl8jC-p6uk2KSkbWjWCL8Qk4rJymCLVg,1688
 bigdl/nano/utils/common/optimizer/__init__.py,sha256=UyRr3ASC_eV9hoDCEDzMWLvWJuozTre9cpCgUmU8U-s,967
 bigdl/nano/utils/common/optimizer/acceleration_option.py,sha256=XD0LLpfoAgIWfTVzrUJRPPOfenyNL2nbOGYDxx5vKgA,3536
 bigdl/nano/utils/common/optimizer/format.py,sha256=3f4QzdysjMqhDwVFJCkB0Ieus8YzBeEy8LsrKwA4UX8,3970
 bigdl/nano/utils/common/optimizer/latency.py,sha256=TJuEqJO3TQoJ4UAZDeOP22fatsmnPMZAdlkxYmPuAGE,4422
 bigdl/nano/utils/common/optimizer/metric.py,sha256=5VpF8vdUoomQnssJo1o7iF6cVMOPO0PQw1sTt29Ykqs,709
 bigdl/nano/utils/common/optimizer/optimizer.py,sha256=PgOem9vDwbhNY9rrzb9VoWBxZFA8y19kUiD6KwlrQH4,8336
-bigdl/nano/utils/pytorch/__init__.py,sha256=ILmdIvLOsBeoGDlGWMIMfAAGo0nHgaJiXjp7l75rneA,1870
+bigdl/nano/utils/pytorch/__init__.py,sha256=GQN0YyzbnQTb-sUyhrZmv_zUQhdPBZGgnbvR4NtCIog,1939
 bigdl/nano/utils/pytorch/attributes.py,sha256=vFxq074vDnw7OWuiQH-9ERJRk3CU719GxBHmWAB_3kQ,1100
 bigdl/nano/utils/pytorch/channel_last.py,sha256=iDbh_s8yFIFq0i1JRyCCYIJf67FRrf7FNU5j1js05NY,4527
 bigdl/nano/utils/pytorch/check_deps.py,sha256=oFq4BvkXQgJdpQBVwYwCd1ZN64Gfu16rmC55AO5KU-0,1807
 bigdl/nano/utils/pytorch/convert.py,sha256=kTQRSN7RZSoPXm7Q77wxUtGVBZ6KKz6pXNuFf36mobA,3358
 bigdl/nano/utils/pytorch/dataloader.py,sha256=pYC4dYKKQLH8XFtCqJ66pvhSyGBSNfM1JFTUuyf6jfQ,6234
 bigdl/nano/utils/pytorch/dataset.py,sha256=8niaxA5SMmet_nVFiO9NMeL4hesCJsJxjhNJwRxfouc,1321
 bigdl/nano/utils/pytorch/input_sample.py,sha256=1wecir3d9uNn39o_StO9GZHr3TMCRC5G4eK4ykIkF-Q,3881
 bigdl/nano/utils/pytorch/inspect.py,sha256=z013opp8CfvOHFFDju2tW9v6vhdjLwQ5huychXYKpiU,4952
+bigdl/nano/utils/pytorch/jit_method.py,sha256=YuSi4LIY828eIFcVJ7VtnLhhkRzVRq_rI0AYMrTQLrk,2595
 bigdl/nano/utils/pytorch/load.py,sha256=LxJ5pQd9Ijfc-KreuihtF1p__Gl25O5a2Noqcm1UHJA,6270
+bigdl/nano/utils/pytorch/metadata.py,sha256=Igl8AXn36WaJGvJWvewaU_CrhGQssDIyImYpqlElkxs,4739
 bigdl/nano/utils/pytorch/metric.py,sha256=7TFBw2F5t_sh-2p0XbbUgUujZULBIwBMFIRt5FV4ATA,1391
 bigdl/nano/utils/pytorch/model_info.py,sha256=IT4RYtgB-YukrEudgu5j3BKDnCBlOIuKo_EyFQuG0G4,5531
 bigdl/nano/utils/pytorch/save.py,sha256=kGCz6w1Dhx4jAgo3A2TAaFv9zL6pxdEeJCJMEnGyX6k,2878
 bigdl/nano/utils/pytorch/version.py,sha256=-zK3eeKIOBgi4HAsUYTumux4axNcx8gYrh61TRm8i-4,1189
 bigdl/nano/utils/pytorch/xpu.py,sha256=anqzDwqy701n-hmSY75G7TrM56WZ6Vdcgbb53q74F6I,1012
 bigdl/nano/utils/tf/__init__.py,sha256=wxh1M-4H8NwLSNhiipdZcXs8gtycV3Cs2DcVPRUnuAE,1057
 bigdl/nano/utils/tf/attributes.py,sha256=AkRSYw0bmdpLwnWZjRXsaIVrDW9H2TJpBB3Tq-RVoGg,4300
 bigdl/nano/utils/tf/backend.py,sha256=CfI_33A-2Na8R462k592F0FlAkwu0lTKzedf_SXyUgQ,2816
 bigdl/nano/utils/tf/data.py,sha256=HyrSXuDeSC-b2X8yP7RBTTKgvLDhV5OqXf7eFOte6nc,7421
 bigdl/nano/utils/tf/preprocess.py,sha256=DHukkdJUa-3z0jK63okaJ0vhbRAJipAoBUpsyS4P93k,2582
 bigdl/nano/utils/tf/subprocess_worker.py,sha256=LitCgfBQdnMqOmGuio4y58Mh9xCoxvn3JVS0INek5mE,1347
 bigdl/nano/utils/tf/version.py,sha256=IN7ZGJJPKSUAJN2LmiJgO_C6cZM8d9023DjcwZ0x9sg,823
-bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init,sha256=A-cN-f_uxGLK3UX7AkwdCpcaMij9SE-rbySsCZRPAPY,12068
-bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-init.ps1,sha256=kX83Ue2ZJgibvddidAHzQ5zhbGlHntKlTcK5lvNmkV4,909
-bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env,sha256=rWzU73m1pflJazC6ulhItByaoudgSa8XlUAJVj3W02c,214
-bigdl_nano-2.3.0b20230417.data/scripts/bigdl-nano-unset-env.ps1,sha256=amIge9q2-1oumygXfW00qeYejEdVMESNUferj5B7txU,127
-bigdl_nano-2.3.0b20230417.dist-info/METADATA,sha256=APNAKu5K9-a9ywFtv17Lf-xh_kA_l2JxcPxhnW1mAeU,10112
-bigdl_nano-2.3.0b20230417.dist-info/WHEEL,sha256=bC8mYJUOJCh5KnyEeT6W_BCQYi3v39D3z64Vy_sFvVg,98
-bigdl_nano-2.3.0b20230417.dist-info/entry_points.txt,sha256=NJqjgi9adpmsUUYPXsd7LzSBdoN1nYjeOyFi9Wih_oU,54
-bigdl_nano-2.3.0b20230417.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
-bigdl_nano-2.3.0b20230417.dist-info/RECORD,,
+bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init,sha256=A-cN-f_uxGLK3UX7AkwdCpcaMij9SE-rbySsCZRPAPY,12068
+bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-init.ps1,sha256=kX83Ue2ZJgibvddidAHzQ5zhbGlHntKlTcK5lvNmkV4,909
+bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env,sha256=rWzU73m1pflJazC6ulhItByaoudgSa8XlUAJVj3W02c,214
+bigdl_nano-2.3.0b20230418.data/scripts/bigdl-nano-unset-env.ps1,sha256=amIge9q2-1oumygXfW00qeYejEdVMESNUferj5B7txU,127
+bigdl_nano-2.3.0b20230418.dist-info/METADATA,sha256=2zEOlI1B4JoPedGH_TgNaHpZ7SLim7U3afJM1Laqqfc,10112
+bigdl_nano-2.3.0b20230418.dist-info/WHEEL,sha256=i9qQj8KaD8_YEW0Vc2oS56fKju23RkQ-FVz-QmzVakQ,98
+bigdl_nano-2.3.0b20230418.dist-info/entry_points.txt,sha256=NJqjgi9adpmsUUYPXsd7LzSBdoN1nYjeOyFi9Wih_oU,54
+bigdl_nano-2.3.0b20230418.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
+bigdl_nano-2.3.0b20230418.dist-info/RECORD,,
```

