# Comparing `tmp/chatgpt_pre_commit_hooks-0.1.1-py3-none-any.whl.zip` & `tmp/chatgpt_pre_commit_hooks-0.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 507206 bytes, number of entries: 15
--rw-r--r--  2.0 unx    80425 b- defN 23-Apr-18 06:13 assets/images/azure-openai-service-key-endpoint.png
--rw-r--r--  2.0 unx   158116 b- defN 23-Apr-18 06:13 assets/images/azure-openai-service-models.png
--rw-r--r--  2.0 unx   161351 b- defN 23-Apr-18 06:13 assets/images/openai-platform-api-key.png
--rw-r--r--  2.0 unx   112924 b- defN 23-Apr-18 06:13 assets/images/openai-platform-org-id.png
--rw-r--r--  2.0 unx       12 b- defN 23-Apr-18 06:13 build/lib/chatgpt_pre_commit_hooks/__init__.py
--rw-r--r--  2.0 unx    11193 b- defN 23-Apr-18 06:13 build/lib/chatgpt_pre_commit_hooks/chatgpt_commit_message.py
--rw-r--r--  2.0 unx       12 b- defN 23-Apr-18 06:13 chatgpt_pre_commit_hooks/__init__.py
--rw-r--r--  2.0 unx    11193 b- defN 23-Apr-18 06:13 chatgpt_pre_commit_hooks/chatgpt_commit_message.py
--rw-r--r--  2.0 unx     4845 b- defN 23-Apr-18 06:13 docs/chatgpt_commit_message.md
--rw-r--r--  2.0 unx     1073 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    14089 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       96 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       48 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1501 b- defN 23-Apr-18 06:14 chatgpt_pre_commit_hooks-0.1.1.dist-info/RECORD
-15 files, 556970 bytes uncompressed, 504644 bytes compressed:  9.4%
+Zip file size: 507237 bytes, number of entries: 15
+-rw-r--r--  2.0 unx    80425 b- defN 23-Apr-18 07:15 assets/images/azure-openai-service-key-endpoint.png
+-rw-r--r--  2.0 unx   158116 b- defN 23-Apr-18 07:15 assets/images/azure-openai-service-models.png
+-rw-r--r--  2.0 unx   161351 b- defN 23-Apr-18 07:15 assets/images/openai-platform-api-key.png
+-rw-r--r--  2.0 unx   112924 b- defN 23-Apr-18 07:15 assets/images/openai-platform-org-id.png
+-rw-r--r--  2.0 unx       12 b- defN 23-Apr-18 07:15 build/lib/chatgpt_pre_commit_hooks/__init__.py
+-rw-r--r--  2.0 unx    11153 b- defN 23-Apr-18 07:15 build/lib/chatgpt_pre_commit_hooks/chatgpt_commit_message.py
+-rw-r--r--  2.0 unx       12 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks/__init__.py
+-rw-r--r--  2.0 unx    11153 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks/chatgpt_commit_message.py
+-rw-r--r--  2.0 unx     4845 b- defN 23-Apr-18 07:15 docs/chatgpt_commit_message.md
+-rw-r--r--  2.0 unx     1073 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13982 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       96 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       48 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1501 b- defN 23-Apr-18 07:15 chatgpt_pre_commit_hooks-0.1.2.dist-info/RECORD
+15 files, 556783 bytes uncompressed, 504675 bytes compressed:  9.4%
```

## zipnote {}

```diff
@@ -21,26 +21,26 @@
 
 Filename: chatgpt_pre_commit_hooks/chatgpt_commit_message.py
 Comment: 
 
 Filename: docs/chatgpt_commit_message.md
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/LICENSE
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/LICENSE
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/METADATA
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/METADATA
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/WHEEL
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/entry_points.txt
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/top_level.txt
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: chatgpt_pre_commit_hooks-0.1.1.dist-info/RECORD
+Filename: chatgpt_pre_commit_hooks-0.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## build/lib/chatgpt_pre_commit_hooks/chatgpt_commit_message.py

```diff
@@ -12,15 +12,15 @@
 from pathlib import Path
 from typing import Dict, List, Optional
 
 import openai
 import tiktoken
 from git.repo import Repo
 
-logger = logging.getLogger(__name__)
+LOG = logging.getLogger(__name__)
 
 
 def get_args() -> argparse.Namespace:
     """Get input arguments."""
     parser = argparse.ArgumentParser()
     parser.add_argument("commit_msg_filename", nargs="?", default=None)  # args.commit_msg_filename
     parser.add_argument("prepare_commit_message_source", nargs="?", default=None)  # args.prepare_commit_message_source
@@ -62,32 +62,32 @@
     - full - if length of diff is less than max_char_count
     - stat - if length of diff is more than max_char_count
     """
     repo = Repo(git_repo_path)
     diff = repo.git.diff(staged=True)
     if len(diff) > max_char_count:
         diff = repo.git.diff(staged=True, stat=True)
-    logger.debug(f"GIT_DIFF: {diff}")
+    LOG.debug(f"GIT_DIFF: {diff}")
     return diff
 
 
 def get_user_commit_message(commit_msg_file_path: str, prepare_commit_message_source: Optional[str]) -> Optional[str]:
     """Get user commit message (if specified)."""
-    logger.debug(f"PREPARE_COMMIT_MESSAGE_SOURCE: {prepare_commit_message_source}")
+    LOG.debug(f"PREPARE_COMMIT_MESSAGE_SOURCE: {prepare_commit_message_source}")
     user_commit_message = None
     if prepare_commit_message_source == "message" or prepare_commit_message_source is None:
         commit_msg_file = Path(commit_msg_file_path)
         commit_msg_file_wrapper = commit_msg_file.open(encoding="utf-8")
         lines = [line for line in commit_msg_file_wrapper.readlines() if not line.startswith("#") and line.strip()]
         commit_msg_file_wrapper.close()
         logging.debug(f"USER_COMMIT_MESSAGE_LINES: {lines}")
         if lines != []:
             user_commit_message = "".join(lines).strip()
 
-    logger.debug(f"USER_COMMIT_MESSAGE: {user_commit_message}")
+    LOG.debug(f"USER_COMMIT_MESSAGE: {user_commit_message}")
 
     if user_commit_message is not None:
         skip_keywords = ["#no-ai", "#no-openai", "#no-chatgpt", "#no-gpt", "#skip-ai", "#skip-openai", "#skip-chatgpt", "#skip-gpt"]
         if any(skip_keyword.casefold() in user_commit_message.casefold() for skip_keyword in skip_keywords):
             logging.debug(f"USER_COMMIT_MESSAGE: {user_commit_message} - SKIPPED")
             sys.exit(0)
 
@@ -127,26 +127,26 @@
 
     role_system.append("Use the present tense.")
     role_system.append("Lines must be at most 72 characters.")
 
     role_system_prompt = " ".join(role_system)
     role_user_prompt = " ".join(role_user)
 
-    logger.debug(f"ROLE_SYSTEM_PROMPT: {role_system_prompt}")
-    logger.debug(f"ROLE_USER_PROMPT: {role_user_prompt}")
+    LOG.debug(f"ROLE_SYSTEM_PROMPT: {role_system_prompt}")
+    LOG.debug(f"ROLE_USER_PROMPT: {role_user_prompt}")
 
     return [
         {"role": "system", "content": role_system_prompt},
         {"role": "user", "content": role_user_prompt},
     ]
 
 
 def get_openai_chat_response(messages: List[Dict[str, str]], args: argparse.Namespace) -> str:
     """Get OpenAI Chat Response."""
-    if logger.isEnabledFor(logging.DEBUG):
+    if LOG.isEnabledFor(logging.DEBUG):
         _num_tokens_from_messages(messages, str(args.openai_model))
         openai.debug = True
 
     openai.api_key = args.openai_api_key
     openai.organization = args.openai_organization
     openai.api_base = args.openai_api_base
     openai.api_type = args.openai_api_type
@@ -157,15 +157,15 @@
     response = openai.ChatCompletion.create(
         model=args.openai_model,
         messages=messages,
         max_tokens=int(args.openai_max_tokens),
         temperature=0,
         top_p=0.1,
     )
-    logger.debug(f"OPENAI_CHAT_RESPONSE: {response}")
+    LOG.debug(f"OPENAI_CHAT_RESPONSE: {response}")
 
     return response["choices"][0]["message"]["content"]
 
 
 def _num_tokens_from_messages(messages: List[Dict[str, str]], model: str) -> int:
     """Return the number of tokens used by a list of messages."""
     try:
@@ -193,48 +193,50 @@
     for message in messages:
         num_tokens += tokens_per_message
         for key, value in message.items():
             num_tokens += len(encoding.encode(value))
             if key == "name":
                 num_tokens += tokens_per_name
     num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
-    logger.debug(f"NUM_TOKENS: {num_tokens}")
+    LOG.debug(f"NUM_TOKENS: {num_tokens}")
     return num_tokens
 
 
 def set_commit_message(commit_msg_file_path: str, commit_msg: str) -> None:
     """Set the suggested commit message."""
     commit_msg_file = Path(commit_msg_file_path)
     commit_msg_file_wrapper = commit_msg_file.open("r+", encoding="utf-8")
     current_content = commit_msg_file_wrapper.read().strip()
     commit_msg_file_wrapper.seek(0, 0)
     commit_msg_file_wrapper.write(f"{commit_msg}\n\n{current_content}")
     commit_msg_file_wrapper.close()
 
 
-def main(args: argparse.Namespace) -> int:
+def main() -> int:
     """Main function of module."""
+    global LOG  # noqa: PLW0603
+
+    args = get_args()
+    LOG = logging.getLogger(__name__)
+    LOG.setLevel(args.log_level.upper())
+
+    if LOG.isEnabledFor(logging.DEBUG):
+        fh = logging.FileHandler(filename="debug.log", mode="w")
+        LOG.addHandler(fh)
+
+    LOG.debug(f"SYS_ARGV: {sys.argv}")
+    LOG.debug(f"ARGS: {args}")
+
     try:
         user_commit_message = get_user_commit_message(args.commit_msg_filename, args.prepare_commit_message_source)
         git_diff = get_git_diff(args.max_char_count, ".")
         openai_chat_prompt_messages = get_openai_chat_prompt_messages(user_commit_message, git_diff, args.emoji, args.description)
         openai_chat_response = get_openai_chat_response(openai_chat_prompt_messages, args)
         set_commit_message(args.commit_msg_filename, openai_chat_response)
     except Exception as error:
         raise Exception(f"Sorry, something went wrong: {error}") from error  # noqa: TRY002
     else:
         return 0
 
 
 if __name__ == "__main__":
-    args = get_args()
-    logger = logging.getLogger(__name__)
-    logger.setLevel(args.log_level.upper())
-
-    if logger.isEnabledFor(logging.DEBUG):
-        fh = logging.FileHandler(filename="debug.log", mode="w")
-        logger.addHandler(fh)
-
-    logger.debug(f"SYS_ARGV: {sys.argv}")
-    logger.debug(f"ARGS: {args}")
-
-    sys.exit(main(args))
+    sys.exit(main())
```

## chatgpt_pre_commit_hooks/chatgpt_commit_message.py

```diff
@@ -12,15 +12,15 @@
 from pathlib import Path
 from typing import Dict, List, Optional
 
 import openai
 import tiktoken
 from git.repo import Repo
 
-logger = logging.getLogger(__name__)
+LOG = logging.getLogger(__name__)
 
 
 def get_args() -> argparse.Namespace:
     """Get input arguments."""
     parser = argparse.ArgumentParser()
     parser.add_argument("commit_msg_filename", nargs="?", default=None)  # args.commit_msg_filename
     parser.add_argument("prepare_commit_message_source", nargs="?", default=None)  # args.prepare_commit_message_source
@@ -62,32 +62,32 @@
     - full - if length of diff is less than max_char_count
     - stat - if length of diff is more than max_char_count
     """
     repo = Repo(git_repo_path)
     diff = repo.git.diff(staged=True)
     if len(diff) > max_char_count:
         diff = repo.git.diff(staged=True, stat=True)
-    logger.debug(f"GIT_DIFF: {diff}")
+    LOG.debug(f"GIT_DIFF: {diff}")
     return diff
 
 
 def get_user_commit_message(commit_msg_file_path: str, prepare_commit_message_source: Optional[str]) -> Optional[str]:
     """Get user commit message (if specified)."""
-    logger.debug(f"PREPARE_COMMIT_MESSAGE_SOURCE: {prepare_commit_message_source}")
+    LOG.debug(f"PREPARE_COMMIT_MESSAGE_SOURCE: {prepare_commit_message_source}")
     user_commit_message = None
     if prepare_commit_message_source == "message" or prepare_commit_message_source is None:
         commit_msg_file = Path(commit_msg_file_path)
         commit_msg_file_wrapper = commit_msg_file.open(encoding="utf-8")
         lines = [line for line in commit_msg_file_wrapper.readlines() if not line.startswith("#") and line.strip()]
         commit_msg_file_wrapper.close()
         logging.debug(f"USER_COMMIT_MESSAGE_LINES: {lines}")
         if lines != []:
             user_commit_message = "".join(lines).strip()
 
-    logger.debug(f"USER_COMMIT_MESSAGE: {user_commit_message}")
+    LOG.debug(f"USER_COMMIT_MESSAGE: {user_commit_message}")
 
     if user_commit_message is not None:
         skip_keywords = ["#no-ai", "#no-openai", "#no-chatgpt", "#no-gpt", "#skip-ai", "#skip-openai", "#skip-chatgpt", "#skip-gpt"]
         if any(skip_keyword.casefold() in user_commit_message.casefold() for skip_keyword in skip_keywords):
             logging.debug(f"USER_COMMIT_MESSAGE: {user_commit_message} - SKIPPED")
             sys.exit(0)
 
@@ -127,26 +127,26 @@
 
     role_system.append("Use the present tense.")
     role_system.append("Lines must be at most 72 characters.")
 
     role_system_prompt = " ".join(role_system)
     role_user_prompt = " ".join(role_user)
 
-    logger.debug(f"ROLE_SYSTEM_PROMPT: {role_system_prompt}")
-    logger.debug(f"ROLE_USER_PROMPT: {role_user_prompt}")
+    LOG.debug(f"ROLE_SYSTEM_PROMPT: {role_system_prompt}")
+    LOG.debug(f"ROLE_USER_PROMPT: {role_user_prompt}")
 
     return [
         {"role": "system", "content": role_system_prompt},
         {"role": "user", "content": role_user_prompt},
     ]
 
 
 def get_openai_chat_response(messages: List[Dict[str, str]], args: argparse.Namespace) -> str:
     """Get OpenAI Chat Response."""
-    if logger.isEnabledFor(logging.DEBUG):
+    if LOG.isEnabledFor(logging.DEBUG):
         _num_tokens_from_messages(messages, str(args.openai_model))
         openai.debug = True
 
     openai.api_key = args.openai_api_key
     openai.organization = args.openai_organization
     openai.api_base = args.openai_api_base
     openai.api_type = args.openai_api_type
@@ -157,15 +157,15 @@
     response = openai.ChatCompletion.create(
         model=args.openai_model,
         messages=messages,
         max_tokens=int(args.openai_max_tokens),
         temperature=0,
         top_p=0.1,
     )
-    logger.debug(f"OPENAI_CHAT_RESPONSE: {response}")
+    LOG.debug(f"OPENAI_CHAT_RESPONSE: {response}")
 
     return response["choices"][0]["message"]["content"]
 
 
 def _num_tokens_from_messages(messages: List[Dict[str, str]], model: str) -> int:
     """Return the number of tokens used by a list of messages."""
     try:
@@ -193,48 +193,50 @@
     for message in messages:
         num_tokens += tokens_per_message
         for key, value in message.items():
             num_tokens += len(encoding.encode(value))
             if key == "name":
                 num_tokens += tokens_per_name
     num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
-    logger.debug(f"NUM_TOKENS: {num_tokens}")
+    LOG.debug(f"NUM_TOKENS: {num_tokens}")
     return num_tokens
 
 
 def set_commit_message(commit_msg_file_path: str, commit_msg: str) -> None:
     """Set the suggested commit message."""
     commit_msg_file = Path(commit_msg_file_path)
     commit_msg_file_wrapper = commit_msg_file.open("r+", encoding="utf-8")
     current_content = commit_msg_file_wrapper.read().strip()
     commit_msg_file_wrapper.seek(0, 0)
     commit_msg_file_wrapper.write(f"{commit_msg}\n\n{current_content}")
     commit_msg_file_wrapper.close()
 
 
-def main(args: argparse.Namespace) -> int:
+def main() -> int:
     """Main function of module."""
+    global LOG  # noqa: PLW0603
+
+    args = get_args()
+    LOG = logging.getLogger(__name__)
+    LOG.setLevel(args.log_level.upper())
+
+    if LOG.isEnabledFor(logging.DEBUG):
+        fh = logging.FileHandler(filename="debug.log", mode="w")
+        LOG.addHandler(fh)
+
+    LOG.debug(f"SYS_ARGV: {sys.argv}")
+    LOG.debug(f"ARGS: {args}")
+
     try:
         user_commit_message = get_user_commit_message(args.commit_msg_filename, args.prepare_commit_message_source)
         git_diff = get_git_diff(args.max_char_count, ".")
         openai_chat_prompt_messages = get_openai_chat_prompt_messages(user_commit_message, git_diff, args.emoji, args.description)
         openai_chat_response = get_openai_chat_response(openai_chat_prompt_messages, args)
         set_commit_message(args.commit_msg_filename, openai_chat_response)
     except Exception as error:
         raise Exception(f"Sorry, something went wrong: {error}") from error  # noqa: TRY002
     else:
         return 0
 
 
 if __name__ == "__main__":
-    args = get_args()
-    logger = logging.getLogger(__name__)
-    logger.setLevel(args.log_level.upper())
-
-    if logger.isEnabledFor(logging.DEBUG):
-        fh = logging.FileHandler(filename="debug.log", mode="w")
-        logger.addHandler(fh)
-
-    logger.debug(f"SYS_ARGV: {sys.argv}")
-    logger.debug(f"ARGS: {args}")
-
-    sys.exit(main(args))
+    sys.exit(main())
```

## docs/chatgpt_commit_message.md

```diff
@@ -21,15 +21,15 @@
 ```
 
 Add to your `.pre-commit-config.yaml`
 
 ```yaml
 repos:
   - repo: https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks
-    rev: v0.1.1 # Use the ref you want to point at, see ‚ö†Ô∏è NOTE below!
+    rev: v0.1.2 # Use the ref you want to point at, see ‚ö†Ô∏è NOTE below!
     hooks:
       - id: chatgpt-commit-message
 ```
 
 > ‚ö†Ô∏è **NOTE**
 >
 > For the `rev:` always try to use the latest version. You can check the latest release under [GitHub Releases](https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks/releases/latest)
@@ -45,15 +45,15 @@
 | `--description`    | bool |  false  | Add short changes summary description to the commit (see, [Commit message with description](https://www.conventionalcommits.org/en/v1.0.0/#commit-message-with-description-and-breaking-change-footer)). Flag type argument, if it exists, it's True. |
 
 Example:
 
 ```yaml
 repos:
   - repo: https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks
-    rev: v0.1.1 # Use the ref you want to point at, see ‚ö†Ô∏è NOTE below!
+    rev: v0.1.2 # Use the ref you want to point at, see ‚ö†Ô∏è NOTE below!
     hooks:
       - id: chatgpt-commit-message
         args:
           - "--emoji"
           - "--max-char-count"
           - "500"
           - "--description"
```

## Comparing `chatgpt_pre_commit_hooks-0.1.1.dist-info/LICENSE` & `chatgpt_pre_commit_hooks-0.1.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `chatgpt_pre_commit_hooks-0.1.1.dist-info/METADATA` & `chatgpt_pre_commit_hooks-0.1.2.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: chatgpt-pre-commit-hooks
-Version: 0.1.1
+Version: 0.1.2
 Summary: Pre-commit hooks collection that utilizes ChatGPT and OpenAI platform to validate modifications made to the codebase.
 Author-email: Dariusz Porowski <3431813+dariuszporowski@users.noreply.github.com>
 License: MIT
 Project-URL: Homepage, https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks/blob/main/README.md
 Project-URL: Bug Tracker, https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks/issues
 Keywords: chatgpt,openai,pre-commit,pre-commit-hooks,pre-commit-hook
 Classifier: Development Status :: 3 - Alpha
@@ -183,15 +183,15 @@
 ```
 
 Example:
 
 ```yaml
 repos:
   - repo: https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks
-    rev: v0.1.1
+    rev: v0.1.2
     hooks:
       - id: chatgpt-commit-message
 ```
 
 > ‚ö†Ô∏è **NOTE**
 >
 > For the `rev:` always try to use the latest version. You can check the latest release under [GitHub Releases](https://github.com/DariuszPorowski/chatgpt-pre-commit-hooks/releases/latest)
@@ -205,15 +205,15 @@
      ```shell
      pip install --upgrade chatgpt-pre-commit-hooks
      ```
 
    - or include it in a `requirements.txt` file in your project:
 
      ```text
-     chatgpt-pre-commit-hooks~=0.1.1
+     chatgpt-pre-commit-hooks~=0.1.2
      ```
 
       and run:
 
      ```shell
      pip install -r requirements.txt
      ```
@@ -235,33 +235,33 @@
 
      ```shell
      poetry add chatgpt-pre-commit-hooks --group dev
      ```
 
 1. Add to your `.pre-commit-config.yaml`
 
-    ```yml
+    ```yaml
     repos:
       - repo: local
         hooks:
-          - id: ... # follow üé£ Hooks section to see available hooks IDs
-            name: Run chatgpt-pre-commit-hooks (<name>)
-            entry: chatgpt-pre-commit-hooks.<id> # follow üé£ Hooks section to see available hooks IDs
+          - id: <id> # follow üé£ Hooks section to see available hooks IDs
+            name: Run <name>
+            entry: <id> # follow üé£ Hooks section to see available hooks IDs
             language: system
     ```
 
     Example:
 
-    ```yml
+    ```yaml
     repos:
-      - repo: local
+    - repo: local
         hooks:
-          - id: chatgpt-commit-message
-            name: Run chatgpt-pre-commit-hooks (chatgpt-commit-message)
-            entry: chatgpt-pre-commit-hooks.chatgpt-commit-message
+      - id: chatgpt-commit-message
+            name: Run ChatGPT commit-message
+            entry: chatgpt-commit-message
             language: system
     ```
 
 ## üõ†Ô∏è Advanced configuration
 
 ### Extra environment variables
```

## Comparing `chatgpt_pre_commit_hooks-0.1.1.dist-info/RECORD` & `chatgpt_pre_commit_hooks-0.1.2.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 assets/images/azure-openai-service-key-endpoint.png,sha256=75WR_RRNmwrK5tZwORqL87fFw330vQM2J2L9MFbGHD0,80425
 assets/images/azure-openai-service-models.png,sha256=EP35fOeRwcwyrcYK8dtyxeNkZpOMkcSXTLuWLCUgvhA,158116
 assets/images/openai-platform-api-key.png,sha256=6l9m2nYx_HQKAVC1erh4WC2bNu56rLx4U-nNL34b2gU,161351
 assets/images/openai-platform-org-id.png,sha256=NHsHvjSry12-ME3EDQwi3RqEqtq5ui4JVTlKrPjVTP0,112924
 build/lib/chatgpt_pre_commit_hooks/__init__.py,sha256=m6UekKftTahNJ3W5K3mZSz4Y4ZZpHRxF_ZAxuaKYL7o,12
-build/lib/chatgpt_pre_commit_hooks/chatgpt_commit_message.py,sha256=Ag8z_ZtMNT-IQCX663QIl9A0C59qGMoXq8PFOxvxpcI,11193
+build/lib/chatgpt_pre_commit_hooks/chatgpt_commit_message.py,sha256=DGdormacelcMiSrZravvu6Hcl9XQsNqwgV55feQGrD8,11153
 chatgpt_pre_commit_hooks/__init__.py,sha256=m6UekKftTahNJ3W5K3mZSz4Y4ZZpHRxF_ZAxuaKYL7o,12
-chatgpt_pre_commit_hooks/chatgpt_commit_message.py,sha256=Ag8z_ZtMNT-IQCX663QIl9A0C59qGMoXq8PFOxvxpcI,11193
-docs/chatgpt_commit_message.md,sha256=RAWGwrX6xpr_nSwPWvDZ0zdRrc1hyspJbM5KN4fH68w,4845
-chatgpt_pre_commit_hooks-0.1.1.dist-info/LICENSE,sha256=Y1yr22x2rPr8UiQmDhuqXBo42CAp2AdW0zoz-8SyAbk,1073
-chatgpt_pre_commit_hooks-0.1.1.dist-info/METADATA,sha256=ngsdZQ0mxnb-VJi_Db1YxPFFX5BYUniPUPehLobz64I,14089
-chatgpt_pre_commit_hooks-0.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-chatgpt_pre_commit_hooks-0.1.1.dist-info/entry_points.txt,sha256=z4xcqjUui6G7p1_OSS8q0eNNT_ww5fDcwS8IjT9K8bc,96
-chatgpt_pre_commit_hooks-0.1.1.dist-info/top_level.txt,sha256=cInhl23HMP-0Ye_EaZZi9C1DjQRdtUusKl9qACyMpoM,48
-chatgpt_pre_commit_hooks-0.1.1.dist-info/RECORD,,
+chatgpt_pre_commit_hooks/chatgpt_commit_message.py,sha256=DGdormacelcMiSrZravvu6Hcl9XQsNqwgV55feQGrD8,11153
+docs/chatgpt_commit_message.md,sha256=1y0CLNv77X1XWsCHulE1jhVg6oaY7uwLpDaNpLNL74s,4845
+chatgpt_pre_commit_hooks-0.1.2.dist-info/LICENSE,sha256=Y1yr22x2rPr8UiQmDhuqXBo42CAp2AdW0zoz-8SyAbk,1073
+chatgpt_pre_commit_hooks-0.1.2.dist-info/METADATA,sha256=A5D0CwZUfL4BULDN334V4h8UiqIDIlb8PqoLHYIDNh0,13982
+chatgpt_pre_commit_hooks-0.1.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+chatgpt_pre_commit_hooks-0.1.2.dist-info/entry_points.txt,sha256=z4xcqjUui6G7p1_OSS8q0eNNT_ww5fDcwS8IjT9K8bc,96
+chatgpt_pre_commit_hooks-0.1.2.dist-info/top_level.txt,sha256=cInhl23HMP-0Ye_EaZZi9C1DjQRdtUusKl9qACyMpoM,48
+chatgpt_pre_commit_hooks-0.1.2.dist-info/RECORD,,
```

